{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "from fancyimpute import KNN    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report, auc, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weekly = pd.read_csv(filepath_or_buffer='Courier_weekly_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courier</th>\n",
       "      <th>week</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3767</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>140.4737</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>2162.4737</td>\n",
       "      <td>0.7632</td>\n",
       "      <td>7.340776</td>\n",
       "      <td>8</td>\n",
       "      <td>20.208158</td>\n",
       "      <td>5.236316</td>\n",
       "      <td>0.8158</td>\n",
       "      <td>43.384804</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3767</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>135.5946</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>2097.4054</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>11.883784</td>\n",
       "      <td>19</td>\n",
       "      <td>18.855405</td>\n",
       "      <td>5.689459</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>35.078042</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3767</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.9767</td>\n",
       "      <td>131.0930</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>2043.8837</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>7.072100</td>\n",
       "      <td>16</td>\n",
       "      <td>18.925116</td>\n",
       "      <td>5.138605</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>31.455285</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3767</td>\n",
       "      <td>6</td>\n",
       "      <td>-22</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>120.1515</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2124.2727</td>\n",
       "      <td>0.7727</td>\n",
       "      <td>7.356567</td>\n",
       "      <td>33</td>\n",
       "      <td>18.259697</td>\n",
       "      <td>4.704394</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>34.252991</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6282</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>4075.7407</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>8.501233</td>\n",
       "      <td>5</td>\n",
       "      <td>26.863704</td>\n",
       "      <td>4.828519</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>46.478114</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6282</td>\n",
       "      <td>3</td>\n",
       "      <td>-20</td>\n",
       "      <td>42</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>113.4821</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>4777.0714</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>8.210125</td>\n",
       "      <td>16</td>\n",
       "      <td>23.651786</td>\n",
       "      <td>5.553571</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>79.407407</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6282</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>105.0000</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>5744.1875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>8.285422</td>\n",
       "      <td>12</td>\n",
       "      <td>18.180937</td>\n",
       "      <td>5.834375</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>87.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6282</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>117.2500</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>4011.7708</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>9.768052</td>\n",
       "      <td>17</td>\n",
       "      <td>20.346667</td>\n",
       "      <td>5.615417</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>85.083333</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6282</td>\n",
       "      <td>6</td>\n",
       "      <td>-12</td>\n",
       "      <td>52</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>107.6389</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>4000.3333</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>7.736114</td>\n",
       "      <td>17</td>\n",
       "      <td>21.941111</td>\n",
       "      <td>5.093056</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>73.904915</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6282</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>124.3750</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>3192.0000</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>7.405355</td>\n",
       "      <td>11</td>\n",
       "      <td>18.591071</td>\n",
       "      <td>5.353214</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>65.618750</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6282</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.9750</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4124.5250</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>7.535005</td>\n",
       "      <td>10</td>\n",
       "      <td>21.523250</td>\n",
       "      <td>4.685000</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>67.201797</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6282</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>0.9362</td>\n",
       "      <td>103.8298</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>4378.7021</td>\n",
       "      <td>0.9149</td>\n",
       "      <td>6.772336</td>\n",
       "      <td>10</td>\n",
       "      <td>19.023404</td>\n",
       "      <td>5.033191</td>\n",
       "      <td>0.8936</td>\n",
       "      <td>61.633191</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6282</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>41</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>4432.4146</td>\n",
       "      <td>0.9756</td>\n",
       "      <td>8.825612</td>\n",
       "      <td>12</td>\n",
       "      <td>18.789024</td>\n",
       "      <td>4.694878</td>\n",
       "      <td>0.9512</td>\n",
       "      <td>60.953472</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10622</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>82</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>2476.2889</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>10.693702</td>\n",
       "      <td>19</td>\n",
       "      <td>24.368889</td>\n",
       "      <td>4.148000</td>\n",
       "      <td>0.6222</td>\n",
       "      <td>29.115176</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10622</td>\n",
       "      <td>1</td>\n",
       "      <td>-12</td>\n",
       "      <td>87</td>\n",
       "      <td>63</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>100.3175</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>2677.4921</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>9.706087</td>\n",
       "      <td>22</td>\n",
       "      <td>18.767143</td>\n",
       "      <td>4.211746</td>\n",
       "      <td>0.6984</td>\n",
       "      <td>42.177203</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13096</td>\n",
       "      <td>5</td>\n",
       "      <td>-10</td>\n",
       "      <td>64</td>\n",
       "      <td>95</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>108.1158</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>4656.7053</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>10.152813</td>\n",
       "      <td>40</td>\n",
       "      <td>17.065895</td>\n",
       "      <td>5.373368</td>\n",
       "      <td>0.7263</td>\n",
       "      <td>83.052951</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13096</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>100.4762</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>4523.8452</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>9.108531</td>\n",
       "      <td>29</td>\n",
       "      <td>18.738929</td>\n",
       "      <td>4.960000</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>85.643004</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13096</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>106.3108</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>3612.4730</td>\n",
       "      <td>0.8784</td>\n",
       "      <td>9.488968</td>\n",
       "      <td>23</td>\n",
       "      <td>22.553108</td>\n",
       "      <td>4.997838</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>73.030398</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>13096</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.9250</td>\n",
       "      <td>105.2250</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>3866.3375</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>9.412291</td>\n",
       "      <td>24</td>\n",
       "      <td>20.007375</td>\n",
       "      <td>5.083250</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>76.819444</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13096</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>80</td>\n",
       "      <td>71</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>111.4930</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>4339.7887</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>9.108214</td>\n",
       "      <td>40</td>\n",
       "      <td>20.889014</td>\n",
       "      <td>5.196901</td>\n",
       "      <td>0.8732</td>\n",
       "      <td>47.096181</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    courier  week  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0      3767     2          6         34         38     0.0789     0.9211   \n",
       "1      3767     4         -1         42         37     0.0000     1.0000   \n",
       "2      3767     5         24         41         43     0.0233     0.9767   \n",
       "3      3767     6        -22         65         66     0.0606     0.9394   \n",
       "4      6282     2          9         33         27     0.0741     0.9259   \n",
       "5      6282     3        -20         42         56     0.0536     0.9464   \n",
       "6      6282     4          9         22         32     0.1250     0.8750   \n",
       "7      6282     5         21         31         48     0.0417     0.9583   \n",
       "8      6282     6        -12         52         72     0.0694     0.9306   \n",
       "9      6282     7          1         40         56     0.0893     0.9107   \n",
       "10     6282     9          5         34         40     0.0250     0.9750   \n",
       "11     6282    10          1         39         47     0.0638     0.9362   \n",
       "12     6282    11         10         40         41     0.0000     1.0000   \n",
       "13    10622     0          5         82         45     0.1111     0.8889   \n",
       "14    10622     1        -12         87         63     0.1270     0.8730   \n",
       "15    13096     5        -10         64         95     0.1684     0.8316   \n",
       "16    13096     6         10         54         84     0.1548     0.8452   \n",
       "17    13096     8          1         53         74     0.0541     0.9459   \n",
       "18    13096     9         26         54         80     0.0750     0.9250   \n",
       "19    13096    10         21         80         71     0.0282     0.9718   \n",
       "\n",
       "    feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "0    140.4737     0.1316  2162.4737     0.7632    7.340776           8   \n",
       "1    135.5946     0.0811  2097.4054     0.9459   11.883784          19   \n",
       "2    131.0930     0.0233  2043.8837     0.9302    7.072100          16   \n",
       "3    120.1515     0.0000  2124.2727     0.7727    7.356567          33   \n",
       "4    100.0000     0.0370  4075.7407     0.8889    8.501233           5   \n",
       "5    113.4821     0.0357  4777.0714     0.9107    8.210125          16   \n",
       "6    105.0000     0.0938  5744.1875     0.8125    8.285422          12   \n",
       "7    117.2500     0.0833  4011.7708     0.9167    9.768052          17   \n",
       "8    107.6389     0.0417  4000.3333     0.8472    7.736114          17   \n",
       "9    124.3750     0.0893  3192.0000     0.9107    7.405355          11   \n",
       "10   100.0000     0.0000  4124.5250     0.9250    7.535005          10   \n",
       "11   103.8298     0.0638  4378.7021     0.9149    6.772336          10   \n",
       "12   100.0000     0.0488  4432.4146     0.9756    8.825612          12   \n",
       "13   100.0000     0.0889  2476.2889     0.5333   10.693702          19   \n",
       "14   100.3175     0.0635  2677.4921     0.7143    9.706087          22   \n",
       "15   108.1158     0.0737  4656.7053     0.7053   10.152813          40   \n",
       "16   100.4762     0.0119  4523.8452     0.6667    9.108531          29   \n",
       "17   106.3108     0.0270  3612.4730     0.8784    9.488968          23   \n",
       "18   105.2250     0.0250  3866.3375     0.8750    9.412291          24   \n",
       "19   111.4930     0.0704  4339.7887     0.8873    9.108214          40   \n",
       "\n",
       "    feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \n",
       "0    20.208158    5.236316      0.8158   43.384804           1          19  \n",
       "1    18.855405    5.689459      0.8919   35.078042           3          11  \n",
       "2    18.925116    5.138605      0.9302   31.455285           1          10  \n",
       "3    18.259697    4.704394      0.7879   34.252991           1          30  \n",
       "4    26.863704    4.828519      0.8889   46.478114           1           4  \n",
       "5    23.651786    5.553571      0.9107   79.407407           2           5  \n",
       "6    18.180937    5.834375      0.7813   87.250000           1           2  \n",
       "7    20.346667    5.615417      0.9167   85.083333           2           9  \n",
       "8    21.941111    5.093056      0.8750   73.904915           1          13  \n",
       "9    18.591071    5.353214      0.9107   65.618750           2           4  \n",
       "10   21.523250    4.685000      0.9250   67.201797           2           3  \n",
       "11   19.023404    5.033191      0.8936   61.633191           1           6  \n",
       "12   18.789024    4.694878      0.9512   60.953472           3           9  \n",
       "13   24.368889    4.148000      0.6222   29.115176           3          18  \n",
       "14   18.767143    4.211746      0.6984   42.177203           2          13  \n",
       "15   17.065895    5.373368      0.7263   83.052951           2          25  \n",
       "16   18.738929    4.960000      0.7143   85.643004           4          13  \n",
       "17   22.553108    4.997838      0.8514   73.030398           5          12  \n",
       "18   20.007375    5.083250      0.8625   76.819444           2          12  \n",
       "19   20.889014    5.196901      0.8732   47.096181           1           9  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly[(weekly.week==11) | (weekly.week==10) | (weekly.week==9)]['courier'].drop_duplicates().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weeks=weekly.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def week_label(row):\n",
    "    courier_set=weekly[(weekly.courier==row['courier']) & ((weekly.week==9) | (weekly.week==10) |(weekly.week==11))]\n",
    "    if courier_set['courier'].count() == 0:\n",
    "        label=1\n",
    "    else: \n",
    "        label=0\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks['label']=weeks.apply(week_label, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weeks=weeks[(weeks.week<8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courier</th>\n",
       "      <th>week</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3767</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>140.4737</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>2162.4737</td>\n",
       "      <td>0.7632</td>\n",
       "      <td>7.340776</td>\n",
       "      <td>8</td>\n",
       "      <td>20.208158</td>\n",
       "      <td>5.236316</td>\n",
       "      <td>0.8158</td>\n",
       "      <td>43.384804</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3767</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>135.5946</td>\n",
       "      <td>0.0811</td>\n",
       "      <td>2097.4054</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>11.883784</td>\n",
       "      <td>19</td>\n",
       "      <td>18.855405</td>\n",
       "      <td>5.689459</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>35.078042</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3767</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>0.9767</td>\n",
       "      <td>131.0930</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>2043.8837</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>7.072100</td>\n",
       "      <td>16</td>\n",
       "      <td>18.925116</td>\n",
       "      <td>5.138605</td>\n",
       "      <td>0.9302</td>\n",
       "      <td>31.455285</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3767</td>\n",
       "      <td>6</td>\n",
       "      <td>-22</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>120.1515</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2124.2727</td>\n",
       "      <td>0.7727</td>\n",
       "      <td>7.356567</td>\n",
       "      <td>33</td>\n",
       "      <td>18.259697</td>\n",
       "      <td>4.704394</td>\n",
       "      <td>0.7879</td>\n",
       "      <td>34.252991</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6282</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.9259</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>4075.7407</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>8.501233</td>\n",
       "      <td>5</td>\n",
       "      <td>26.863704</td>\n",
       "      <td>4.828519</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>46.478114</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6282</td>\n",
       "      <td>3</td>\n",
       "      <td>-20</td>\n",
       "      <td>42</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>113.4821</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>4777.0714</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>8.210125</td>\n",
       "      <td>16</td>\n",
       "      <td>23.651786</td>\n",
       "      <td>5.553571</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>79.407407</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6282</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>105.0000</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>5744.1875</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>8.285422</td>\n",
       "      <td>12</td>\n",
       "      <td>18.180937</td>\n",
       "      <td>5.834375</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>87.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6282</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>48</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>117.2500</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>4011.7708</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>9.768052</td>\n",
       "      <td>17</td>\n",
       "      <td>20.346667</td>\n",
       "      <td>5.615417</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>85.083333</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6282</td>\n",
       "      <td>6</td>\n",
       "      <td>-12</td>\n",
       "      <td>52</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0694</td>\n",
       "      <td>0.9306</td>\n",
       "      <td>107.6389</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>4000.3333</td>\n",
       "      <td>0.8472</td>\n",
       "      <td>7.736114</td>\n",
       "      <td>17</td>\n",
       "      <td>21.941111</td>\n",
       "      <td>5.093056</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>73.904915</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6282</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>124.3750</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>3192.0000</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>7.405355</td>\n",
       "      <td>11</td>\n",
       "      <td>18.591071</td>\n",
       "      <td>5.353214</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>65.618750</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10622</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>82</td>\n",
       "      <td>45</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>2476.2889</td>\n",
       "      <td>0.5333</td>\n",
       "      <td>10.693702</td>\n",
       "      <td>19</td>\n",
       "      <td>24.368889</td>\n",
       "      <td>4.148000</td>\n",
       "      <td>0.6222</td>\n",
       "      <td>29.115176</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10622</td>\n",
       "      <td>1</td>\n",
       "      <td>-12</td>\n",
       "      <td>87</td>\n",
       "      <td>63</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.8730</td>\n",
       "      <td>100.3175</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>2677.4921</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>9.706087</td>\n",
       "      <td>22</td>\n",
       "      <td>18.767143</td>\n",
       "      <td>4.211746</td>\n",
       "      <td>0.6984</td>\n",
       "      <td>42.177203</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13096</td>\n",
       "      <td>5</td>\n",
       "      <td>-10</td>\n",
       "      <td>64</td>\n",
       "      <td>95</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>0.8316</td>\n",
       "      <td>108.1158</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>4656.7053</td>\n",
       "      <td>0.7053</td>\n",
       "      <td>10.152813</td>\n",
       "      <td>40</td>\n",
       "      <td>17.065895</td>\n",
       "      <td>5.373368</td>\n",
       "      <td>0.7263</td>\n",
       "      <td>83.052951</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13096</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>84</td>\n",
       "      <td>0.1548</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>100.4762</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>4523.8452</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>9.108531</td>\n",
       "      <td>29</td>\n",
       "      <td>18.738929</td>\n",
       "      <td>4.960000</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>85.643004</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>14261</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>79</td>\n",
       "      <td>59</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>123.5763</td>\n",
       "      <td>0.1695</td>\n",
       "      <td>3788.9831</td>\n",
       "      <td>0.8136</td>\n",
       "      <td>9.361863</td>\n",
       "      <td>21</td>\n",
       "      <td>19.361525</td>\n",
       "      <td>5.578136</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>41.786920</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14261</td>\n",
       "      <td>1</td>\n",
       "      <td>-16</td>\n",
       "      <td>83</td>\n",
       "      <td>72</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>112.6528</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>3706.1944</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>9.353240</td>\n",
       "      <td>29</td>\n",
       "      <td>20.061528</td>\n",
       "      <td>5.398194</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>44.115462</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>14261</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.9623</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.1509</td>\n",
       "      <td>4125.6415</td>\n",
       "      <td>0.8868</td>\n",
       "      <td>11.751260</td>\n",
       "      <td>27</td>\n",
       "      <td>20.421132</td>\n",
       "      <td>4.909245</td>\n",
       "      <td>0.9245</td>\n",
       "      <td>45.108624</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14261</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>76</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.9868</td>\n",
       "      <td>115.9079</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>4456.1447</td>\n",
       "      <td>0.9342</td>\n",
       "      <td>10.395178</td>\n",
       "      <td>34</td>\n",
       "      <td>19.920263</td>\n",
       "      <td>5.792895</td>\n",
       "      <td>0.9605</td>\n",
       "      <td>59.387279</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>14261</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>117.3750</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>3858.7955</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>9.110610</td>\n",
       "      <td>32</td>\n",
       "      <td>23.058523</td>\n",
       "      <td>5.457386</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>62.532793</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14261</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>79</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>112.4286</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>3520.1299</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>10.404332</td>\n",
       "      <td>39</td>\n",
       "      <td>21.721299</td>\n",
       "      <td>5.364286</td>\n",
       "      <td>0.8442</td>\n",
       "      <td>50.335795</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    courier  week  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0      3767     2          6         34         38     0.0789     0.9211   \n",
       "1      3767     4         -1         42         37     0.0000     1.0000   \n",
       "2      3767     5         24         41         43     0.0233     0.9767   \n",
       "3      3767     6        -22         65         66     0.0606     0.9394   \n",
       "4      6282     2          9         33         27     0.0741     0.9259   \n",
       "5      6282     3        -20         42         56     0.0536     0.9464   \n",
       "6      6282     4          9         22         32     0.1250     0.8750   \n",
       "7      6282     5         21         31         48     0.0417     0.9583   \n",
       "8      6282     6        -12         52         72     0.0694     0.9306   \n",
       "9      6282     7          1         40         56     0.0893     0.9107   \n",
       "13    10622     0          5         82         45     0.1111     0.8889   \n",
       "14    10622     1        -12         87         63     0.1270     0.8730   \n",
       "15    13096     5        -10         64         95     0.1684     0.8316   \n",
       "16    13096     6         10         54         84     0.1548     0.8452   \n",
       "21    14261     0          4         79         59     0.0339     0.9661   \n",
       "22    14261     1        -16         83         72     0.0139     0.9861   \n",
       "23    14261     2          2         67         53     0.0377     0.9623   \n",
       "24    14261     3          3         69         76     0.0132     0.9868   \n",
       "25    14261     4          7         72         88     0.0227     0.9773   \n",
       "26    14261     5         -1         79         77     0.0260     0.9740   \n",
       "\n",
       "    feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "0    140.4737     0.1316  2162.4737     0.7632    7.340776           8   \n",
       "1    135.5946     0.0811  2097.4054     0.9459   11.883784          19   \n",
       "2    131.0930     0.0233  2043.8837     0.9302    7.072100          16   \n",
       "3    120.1515     0.0000  2124.2727     0.7727    7.356567          33   \n",
       "4    100.0000     0.0370  4075.7407     0.8889    8.501233           5   \n",
       "5    113.4821     0.0357  4777.0714     0.9107    8.210125          16   \n",
       "6    105.0000     0.0938  5744.1875     0.8125    8.285422          12   \n",
       "7    117.2500     0.0833  4011.7708     0.9167    9.768052          17   \n",
       "8    107.6389     0.0417  4000.3333     0.8472    7.736114          17   \n",
       "9    124.3750     0.0893  3192.0000     0.9107    7.405355          11   \n",
       "13   100.0000     0.0889  2476.2889     0.5333   10.693702          19   \n",
       "14   100.3175     0.0635  2677.4921     0.7143    9.706087          22   \n",
       "15   108.1158     0.0737  4656.7053     0.7053   10.152813          40   \n",
       "16   100.4762     0.0119  4523.8452     0.6667    9.108531          29   \n",
       "21   123.5763     0.1695  3788.9831     0.8136    9.361863          21   \n",
       "22   112.6528     0.1111  3706.1944     0.9167    9.353240          29   \n",
       "23   100.0000     0.1509  4125.6415     0.8868   11.751260          27   \n",
       "24   115.9079     0.1447  4456.1447     0.9342   10.395178          34   \n",
       "25   117.3750     0.2273  3858.7955     0.9432    9.110610          32   \n",
       "26   112.4286     0.0390  3520.1299     0.8571   10.404332          39   \n",
       "\n",
       "    feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \\\n",
       "0    20.208158    5.236316      0.8158   43.384804           1          19   \n",
       "1    18.855405    5.689459      0.8919   35.078042           3          11   \n",
       "2    18.925116    5.138605      0.9302   31.455285           1          10   \n",
       "3    18.259697    4.704394      0.7879   34.252991           1          30   \n",
       "4    26.863704    4.828519      0.8889   46.478114           1           4   \n",
       "5    23.651786    5.553571      0.9107   79.407407           2           5   \n",
       "6    18.180937    5.834375      0.7813   87.250000           1           2   \n",
       "7    20.346667    5.615417      0.9167   85.083333           2           9   \n",
       "8    21.941111    5.093056      0.8750   73.904915           1          13   \n",
       "9    18.591071    5.353214      0.9107   65.618750           2           4   \n",
       "13   24.368889    4.148000      0.6222   29.115176           3          18   \n",
       "14   18.767143    4.211746      0.6984   42.177203           2          13   \n",
       "15   17.065895    5.373368      0.7263   83.052951           2          25   \n",
       "16   18.738929    4.960000      0.7143   85.643004           4          13   \n",
       "21   19.361525    5.578136      0.8475   41.786920           2           4   \n",
       "22   20.061528    5.398194      0.9167   44.115462           3           5   \n",
       "23   20.421132    4.909245      0.9245   45.108624           3          10   \n",
       "24   19.920263    5.792895      0.9605   59.387279           3           2   \n",
       "25   23.058523    5.457386      0.9091   62.532793           4           6   \n",
       "26   21.721299    5.364286      0.8442   50.335795           3          13   \n",
       "\n",
       "    label  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       0  \n",
       "5       0  \n",
       "6       0  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  \n",
       "13      1  \n",
       "14      1  \n",
       "15      0  \n",
       "16      0  \n",
       "21      1  \n",
       "22      1  \n",
       "23      1  \n",
       "24      1  \n",
       "25      1  \n",
       "26      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeks.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_consecutive(vector):\n",
    "    longest = 0\n",
    "    current = 0\n",
    "    for num in vector:\n",
    "        if num == 1:\n",
    "            current += 1\n",
    "        else:\n",
    "            longest = max(longest, current)\n",
    "            current = 0\n",
    "\n",
    "    return max(longest, current)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_consecutive([0,1,1,1,0,0,1,0,1,1,1,1,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_streak(series):\n",
    "    week_vector=[0,0,0,0,0,0,0,0]\n",
    "    \n",
    "    for i in series:\n",
    "        week_vector[i]=1\n",
    "    streak=max_consecutive(week_vector)\n",
    "    return streak\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lifetime_mean(series):\n",
    "    return np.sum(series)/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold=weeks.groupby(['courier','label'], as_index=False ).agg({'week':['count', max_streak],'feature_1':['mean',lifetime_mean],'feature_2':['mean',lifetime_mean],'feature_3':['mean',lifetime_mean],'feature_4':['mean',lifetime_mean],'feature_5':['mean',lifetime_mean],'feature_6':['mean',lifetime_mean],'feature_7':['mean',lifetime_mean],'feature_8':['mean',lifetime_mean]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>courier</th>\n",
       "      <th>week</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "      <th>feature_11</th>\n",
       "      <th>feature_12</th>\n",
       "      <th>feature_13</th>\n",
       "      <th>feature_14</th>\n",
       "      <th>feature_15</th>\n",
       "      <th>feature_16</th>\n",
       "      <th>feature_17</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>101552</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>96</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.9375</td>\n",
       "      <td>115.4167</td>\n",
       "      <td>0.1146</td>\n",
       "      <td>2526.3125</td>\n",
       "      <td>0.8646</td>\n",
       "      <td>9.938890</td>\n",
       "      <td>38</td>\n",
       "      <td>19.448333</td>\n",
       "      <td>4.831042</td>\n",
       "      <td>0.8542</td>\n",
       "      <td>47.183007</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>101552</td>\n",
       "      <td>1</td>\n",
       "      <td>-12</td>\n",
       "      <td>102</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.9694</td>\n",
       "      <td>122.8469</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>2602.7653</td>\n",
       "      <td>0.8265</td>\n",
       "      <td>10.782992</td>\n",
       "      <td>41</td>\n",
       "      <td>19.302449</td>\n",
       "      <td>5.267857</td>\n",
       "      <td>0.8673</td>\n",
       "      <td>44.454521</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>101552</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0737</td>\n",
       "      <td>0.9263</td>\n",
       "      <td>110.0632</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>2614.3158</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>10.320528</td>\n",
       "      <td>37</td>\n",
       "      <td>23.150000</td>\n",
       "      <td>4.762842</td>\n",
       "      <td>0.8842</td>\n",
       "      <td>48.739506</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>101552</td>\n",
       "      <td>3</td>\n",
       "      <td>-9</td>\n",
       "      <td>95</td>\n",
       "      <td>88</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>113.5682</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>2583.0455</td>\n",
       "      <td>0.9205</td>\n",
       "      <td>9.580684</td>\n",
       "      <td>33</td>\n",
       "      <td>22.335909</td>\n",
       "      <td>4.818295</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>39.243567</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>101552</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>86</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>121.9467</td>\n",
       "      <td>0.2667</td>\n",
       "      <td>2316.0000</td>\n",
       "      <td>0.9333</td>\n",
       "      <td>9.942443</td>\n",
       "      <td>30</td>\n",
       "      <td>21.335467</td>\n",
       "      <td>4.931467</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>37.920543</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>101552</td>\n",
       "      <td>5</td>\n",
       "      <td>-13</td>\n",
       "      <td>100</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.9346</td>\n",
       "      <td>118.7477</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>2363.8318</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>8.228350</td>\n",
       "      <td>38</td>\n",
       "      <td>21.186168</td>\n",
       "      <td>4.572243</td>\n",
       "      <td>0.8318</td>\n",
       "      <td>43.220833</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>101552</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.9589</td>\n",
       "      <td>122.3973</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>2294.2329</td>\n",
       "      <td>0.8767</td>\n",
       "      <td>8.809360</td>\n",
       "      <td>51</td>\n",
       "      <td>23.320411</td>\n",
       "      <td>4.752466</td>\n",
       "      <td>0.8356</td>\n",
       "      <td>31.955300</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>101552</td>\n",
       "      <td>7</td>\n",
       "      <td>-6</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.9625</td>\n",
       "      <td>110.1375</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>2481.0875</td>\n",
       "      <td>0.8500</td>\n",
       "      <td>8.936045</td>\n",
       "      <td>52</td>\n",
       "      <td>19.992125</td>\n",
       "      <td>4.389875</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>32.114722</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     courier  week  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "151   101552     0         17         85         96     0.0625     0.9375   \n",
       "152   101552     1        -12        102         98     0.0306     0.9694   \n",
       "153   101552     2          5         90         95     0.0737     0.9263   \n",
       "154   101552     3         -9         95         88     0.0341     0.9659   \n",
       "155   101552     4         14         86         75     0.0000     1.0000   \n",
       "156   101552     5        -13        100        107     0.0654     0.9346   \n",
       "157   101552     6         13         87         73     0.0411     0.9589   \n",
       "158   101552     7         -6        100         80     0.0375     0.9625   \n",
       "\n",
       "     feature_6  feature_7  feature_8  feature_9  feature_10  feature_11  \\\n",
       "151   115.4167     0.1146  2526.3125     0.8646    9.938890          38   \n",
       "152   122.8469     0.1735  2602.7653     0.8265   10.782992          41   \n",
       "153   110.0632     0.2000  2614.3158     0.8632   10.320528          37   \n",
       "154   113.5682     0.1818  2583.0455     0.9205    9.580684          33   \n",
       "155   121.9467     0.2667  2316.0000     0.9333    9.942443          30   \n",
       "156   118.7477     0.0374  2363.8318     0.8318    8.228350          38   \n",
       "157   122.3973     0.0137  2294.2329     0.8767    8.809360          51   \n",
       "158   110.1375     0.0500  2481.0875     0.8500    8.936045          52   \n",
       "\n",
       "     feature_12  feature_13  feature_14  feature_15  feature_16  feature_17  \\\n",
       "151   19.448333    4.831042      0.8542   47.183007           5          12   \n",
       "152   19.302449    5.267857      0.8673   44.454521           4          23   \n",
       "153   23.150000    4.762842      0.8842   48.739506           4          21   \n",
       "154   22.335909    4.818295      0.8977   39.243567           1          19   \n",
       "155   21.335467    4.931467      1.0000   37.920543           2           6   \n",
       "156   21.186168    4.572243      0.8318   43.220833           5          17   \n",
       "157   23.320411    4.752466      0.8356   31.955300           2          10   \n",
       "158   19.992125    4.389875      0.8625   32.114722           1          17   \n",
       "\n",
       "     label  \n",
       "151      0  \n",
       "152      0  \n",
       "153      0  \n",
       "154      0  \n",
       "155      0  \n",
       "156      0  \n",
       "157      0  \n",
       "158      0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weeks[(weeks.courier==101552)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold.columns = [gold + '_' + i for gold, i in zip(gold.columns.get_level_values(0), gold.columns.get_level_values(1).astype(str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold=gold.rename(columns={'courier_':'courier','label_':'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['courier', 'y', 'week_count', 'week_max_streak', 'feature_1_mean',\n",
       "       'feature_1_lifetime_mean', 'feature_2_mean', 'feature_2_lifetime_mean',\n",
       "       'feature_3_mean', 'feature_3_lifetime_mean', 'feature_4_mean',\n",
       "       'feature_4_lifetime_mean', 'feature_5_mean', 'feature_5_lifetime_mean',\n",
       "       'feature_6_mean', 'feature_6_lifetime_mean', 'feature_7_mean',\n",
       "       'feature_7_lifetime_mean', 'feature_8_mean', 'feature_8_lifetime_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold.set_index('courier', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "golden_x= gold.drop(['y'], axis=1)\n",
    "golden_y= gold['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(golden_x,golden_y , test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    365\n",
       "1    364\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_y.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [-0.00539862]\n",
      "Regression: [[-8.28215518e-02  4.58555275e-01 -2.48117531e-02 -1.32445437e-01\n",
      "  -4.26974532e-03  4.50452698e-03 -3.66007369e-02 -5.32518367e-03\n",
      "   1.23170008e-02  1.05280560e-02 -1.77169318e-02 -2.08815932e-02\n",
      "   2.39859665e-02 -5.20528515e-02 -5.96401155e-02 -2.03003502e-02\n",
      "  -2.34751120e-06  2.74612884e-04]]\n",
      "Accuracy of logistic regression classifier on test set: 0.78\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.80      0.79        93\n",
      "          1       0.78      0.77      0.78        90\n",
      "\n",
      "avg / total       0.78      0.78      0.78       183\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(X=x_train, y=y_train)\n",
    "test_y_pred = logisticRegr.predict(x_test)\n",
    "cf_mt = confusion_matrix(y_test, test_y_pred)\n",
    "print('Intercept: ' + str(logisticRegr.intercept_))\n",
    "print('Regression: ' + str(logisticRegr.coef_))\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logisticRegr.score(x_test, y_test)))\n",
    "print(classification_report(y_test, test_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,15,'Predicted label')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEoCAYAAACzVD1FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nycHMzLI4OZiZWRYnBzMzy/L/WCKfYariv1IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e24716898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_df = pd.DataFrame(cf_mt, ('False', 'True'), ('False', 'True'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "plt.ylabel('True label', fontsize = 14)\n",
    "plt.xlabel('Predicted label', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7814207650273224\n",
      "Accuracy: 0.775 (0.069)\n",
      "Logloss: -0.482 (0.152)\n",
      "AUC: 0.875 (0.077)\n",
      "[[74 19]\n",
      " [21 69]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,15,'Predicted label')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEoCAYAAACzVD1FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nycHMzLI4OZiZWRYnBzMzy/L/WCKfYariv1IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e247167f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)\n",
    "print(clf.score(x_test, y_test))\n",
    "seed = 7\n",
    "k_fold = KFold(n_splits=10, random_state=seed)\n",
    "scoring = 'accuracy'\n",
    "results = results=cross_val_score(clf, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "scoring = 'neg_log_loss'\n",
    "results = results=cross_val_score(clf, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "scoring = 'roc_auc'\n",
    "results = results=cross_val_score(clf, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "clf.fit(x_train,y_train)\n",
    "predicted=clf.predict(x_test)\n",
    "matrix = confusion_matrix(y_test, predicted)\n",
    "print(matrix)\n",
    "\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(matrix, ('False', 'True'), ('False', 'True'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "plt.ylabel('True label', fontsize = 14)\n",
    "plt.xlabel('Predicted label', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e25a15438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = clf.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7814207650273224\n",
      "Accuracy: 0.782 (0.060)\n",
      "Logloss: -0.467 (0.119)\n",
      "AUC: 0.879 (0.072)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,15,'Predicted label')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEoCAYAAACzVD1FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nycHMzLI4OZiZWRYnBzMzy/L/WCKfYariv1IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e23f4c710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "penalty = ['l2']\n",
    "#C = np.logspace(0, 4, 10)\n",
    "C = uniform(loc=0, scale=4)\n",
    "hyperparameters = dict(C=C, penalty=penalty)\n",
    "\n",
    "#clf_CV = GridSearchCV(clf, hyperparameters, cv=5, verbose=0)\n",
    "clf_CV = RandomizedSearchCV(clf, hyperparameters, random_state=1, n_iter=10, cv=5, verbose=0, n_jobs=-1)\n",
    "best_model = clf_CV.fit(x_train, y_train)\n",
    "clf_CV.fit(x_train,y_train)\n",
    "print(clf_CV.score(x_test, y_test))\n",
    "seed = 7\n",
    "k_fold = KFold(n_splits=10, random_state=seed)\n",
    "scoring = 'accuracy'\n",
    "results = results=cross_val_score(clf_CV, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "scoring = 'neg_log_loss'\n",
    "results = results=cross_val_score(clf_CV, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "scoring = 'roc_auc'\n",
    "results = results=cross_val_score(clf_CV, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "predicted=best_model.predict(x_test)\n",
    "matrix = confusion_matrix(y_test, predicted)\n",
    "\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(matrix, ('False', 'True'), ('False', 'True'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "plt.ylabel('True label', fontsize = 14)\n",
    "plt.xlabel('Predicted label', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7814207650273224\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e25a4ce48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "probs = best_model.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(accuracy_score(y_test, predicted))\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': [3, 5, 10], \n",
    "    'n_estimators' : [100, 300],\n",
    "    'max_depth': [3, 5, 15, 25],\n",
    "    'max_features': [3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search_wrapper(refit_score='precision_score'):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifier using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n",
    "                           cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    grid_search.fit(x_train.values, y_train.values)\n",
    "\n",
    "    \n",
    "    y_pred = grid_search.predict(x_test.values)\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    # confusion matrix on the test data.\n",
    "    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred),\n",
    "                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibException",
     "evalue": "JoblibException\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x0000028E180A6DB0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anacon...ges\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\h...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x0000028E180A6DB0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anacon...ges\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\h...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 3, 29, 14, 17, 11, 74099, tzinfo=tzutc()), 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'session': 'B6D358B33BA14A83935DFEDC7485FFB8', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'B6D358B33BA14A83935DFEDC7485FFB8']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 3, 29, 14, 17, 11, 74099, tzinfo=tzutc()), 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'session': 'B6D358B33BA14A83935DFEDC7485FFB8', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'B6D358B33BA14A83935DFEDC7485FFB8'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 3, 29, 14, 17, 11, 74099, tzinfo=tzutc()), 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'session': 'B6D358B33BA14A83935DFEDC7485FFB8', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>], cell_name='<ipython-input-78-bc150bd7f799>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 28e24146d68, executio..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x0000028E2470E930, file \"<ipython-input-78-bc150bd7f799>\", line 1>\n        result = <ExecutionResult object at 28e24146d68, executio..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x0000028E2470E930, file \"<ipython-input-78-bc150bd7f799>\", line 1>, result=<ExecutionResult object at 28e24146d68, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x0000028E2470E930, file \"<ipython-input-78-bc150bd7f799>\", line 1>\n        self.user_global_ns = {'C': <scipy.stats._distn_infrastructure.rv_frozen object>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...accuracy_score, precision_score, confusion_matrix', \"get_ipython().magic('matplotlib inline')\", \"weekly = pd.read_csv(filepath_or_buffer='Courier_weekly_data.csv')\", 'weekly.head(20)', \"weekly[(weekly.week==11) | (weekly.week==10) | (...ly.week==9)]['courier'].drop_duplicates().count()\", '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', 'weeks=weekly.copy()', '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', \"week['label']=week.apply(week_label, axis=1)\", \"weeks['label']=weeks.apply(week_label, axis=1)\", 'weeks=weeks[(weeks.week<8)]', 'weeks.head()', 'def max_consecutive(vector):\\n    longest = 0\\n   ...urrent = 0\\n\\n    return max(longest, current)\\n    ', 'def max_streak(series):\\n    week_vector=[0,0,0,0..._vector)\\n    return streak\\n    \\n        \\n        ', \"gold=weeks.groupby(['courier'], as_index=False ).agg({week:[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':['count', max_consecutive]})\", 'gold', 'weeks.head(20)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNN': <class 'fancyimpute.knn.KNN'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:     courier  week  feature_1  feature_2  feature...     0.8732   47.096181           1           9  , 5: 387, 12:    courier  week  feature_1  feature_2  feature_...  1  \n1      1  \n2      1  \n3      1  \n4      0  , 18:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 19:     courier  week  feature_1  feature_2  feature... \n23      1  \n24      1  \n25      1  \n26      1  , 20: 4, 21: 3, 22: 6, 25:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 28:     courier  week           \n            count m...  519341     3          2\n\n[729 rows x 3 columns], ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, ...}\n        self.user_ns = {'C': <scipy.stats._distn_infrastructure.rv_frozen object>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...accuracy_score, precision_score, confusion_matrix', \"get_ipython().magic('matplotlib inline')\", \"weekly = pd.read_csv(filepath_or_buffer='Courier_weekly_data.csv')\", 'weekly.head(20)', \"weekly[(weekly.week==11) | (weekly.week==10) | (...ly.week==9)]['courier'].drop_duplicates().count()\", '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', 'weeks=weekly.copy()', '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', \"week['label']=week.apply(week_label, axis=1)\", \"weeks['label']=weeks.apply(week_label, axis=1)\", 'weeks=weeks[(weeks.week<8)]', 'weeks.head()', 'def max_consecutive(vector):\\n    longest = 0\\n   ...urrent = 0\\n\\n    return max(longest, current)\\n    ', 'def max_streak(series):\\n    week_vector=[0,0,0,0..._vector)\\n    return streak\\n    \\n        \\n        ', \"gold=weeks.groupby(['courier'], as_index=False ).agg({week:[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':['count', max_consecutive]})\", 'gold', 'weeks.head(20)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNN': <class 'fancyimpute.knn.KNN'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:     courier  week  feature_1  feature_2  feature...     0.8732   47.096181           1           9  , 5: 387, 12:    courier  week  feature_1  feature_2  feature_...  1  \n1      1  \n2      1  \n3      1  \n4      0  , 18:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 19:     courier  week  feature_1  feature_2  feature... \n23      1  \n24      1  \n25      1  \n26      1  , 20: 4, 21: 3, 22: 6, 25:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 28:     courier  week           \n            count m...  519341     3          2\n\n[729 rows x 3 columns], ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<ipython-input-78-bc150bd7f799> in <module>()\n----> 1 grid_search_clf = grid_search_wrapper(refit_score='precision_score')\n\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<ipython-input-77-469f35795da2> in grid_search_wrapper(refit_score='precision_score')\n      4     prints classifier performance metrics\n      5     \"\"\"\n      6     skf = StratifiedKFold(n_splits=10)\n      7     grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n      8                            cv=skf, return_train_score=True, n_jobs=-1)\n----> 9     grid_search.fit(x_train.values, y_train.values)\n     10 \n     11     \n     12     y_pred = grid_search.predict(x_test.values)\n     13 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=StratifiedKFold(n_splits=10, ran...: make_scorer(accuracy_score)},\n       verbose=0), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...d(n_splits=10, random_state=None, shuffle=False)>\n        X = array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y = array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64)\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nJoblibValueError                                   Fri Mar 29 15:18:23 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    735 %s\"\"\" % (this_report, exception.message)\n    736                     # Convert this to a JoblibException\n    737                     exception_type = _mk_exception(exception.etype)[0]\n    738                     exception = exception_type(report)\n    739 \n--> 740                     raise exception\n        exception = undefined\n    741 \n    742     def __call__(self, iterable):\n    743         if self._jobs:\n    744             raise ValueError('This Parallel instance is already running')\n\nJoblibValueError: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<string> in <module>()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in spawn_main(pipe_handle=2628, parent_pid=15660, tracker_fd=None)\n    100         fd = msvcrt.open_osfhandle(new_handle, os.O_RDONLY)\n    101     else:\n    102         from . import semaphore_tracker\n    103         semaphore_tracker._semaphore_tracker._fd = tracker_fd\n    104         fd = pipe_handle\n--> 105     exitcode = _main(fd)\n        exitcode = undefined\n        fd = 3\n    106     sys.exit(exitcode)\n    107 \n    108 \n    109 def _main(fd):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in _main(fd=3)\n    113             preparation_data = reduction.pickle.load(from_parent)\n    114             prepare(preparation_data)\n    115             self = reduction.pickle.load(from_parent)\n    116         finally:\n    117             del process.current_process()._inheriting\n--> 118     return self._bootstrap()\n        self._bootstrap = <bound method BaseProcess._bootstrap of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    119 \n    120 \n    121 def _check_not_importing_main():\n    122     if getattr(process.current_process(), '_inheriting', False):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in _bootstrap(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n    253                 # delay finalization of the old process object until after\n    254                 # _run_after_forkers() is executed\n    255                 del old_process\n    256             util.info('child process calling self.run()')\n    257             try:\n--> 258                 self.run()\n        self.run = <bound method BaseProcess.run of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    259                 exitcode = 0\n    260             finally:\n    261                 util._exit_function()\n    262         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in run(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, <sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py in worker(inqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, outqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = None\n        func = <sklearn.externals.joblib._parallel_backends.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception and func is not _helper_reraises_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py in __call__(self=<sklearn.externals.joblib._parallel_backends.SafeFunction object>, *args=(), **kwargs={})\n    345     def __init__(self, func):\n    346         self.func = func\n    347 \n    348     def __call__(self, *args, **kwargs):\n    349         try:\n--> 350             return self.func(*args, **kwargs)\n        self.func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    351         except KeyboardInterrupt:\n    352             # We capture the KeyboardInterrupt and reraise it as\n    353             # something different, as multiprocessing does not\n    354             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Mar 29 15:18:21 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...        random_state=815305645, splitter='best')>\n        X = array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\", line 121, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\", line 790, in fit\n    X_idx_sorted=X_idx_sorted)\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\", line 242, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 699, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\", line 644, in get\n    raise self._value\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Mar 29 15:18:21 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...        random_state=815305645, splitter='best')>\n        X = array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\", line 328, in fit\n    for i, t in enumerate(trees))\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 789, in __call__\n    self.retrieve()\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 740, in retrieve\n    raise exception\nsklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<string> in <module>()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in spawn_main(pipe_handle=2628, parent_pid=15660, tracker_fd=None)\n    100         fd = msvcrt.open_osfhandle(new_handle, os.O_RDONLY)\n    101     else:\n    102         from . import semaphore_tracker\n    103         semaphore_tracker._semaphore_tracker._fd = tracker_fd\n    104         fd = pipe_handle\n--> 105     exitcode = _main(fd)\n        exitcode = undefined\n        fd = 3\n    106     sys.exit(exitcode)\n    107 \n    108 \n    109 def _main(fd):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in _main(fd=3)\n    113             preparation_data = reduction.pickle.load(from_parent)\n    114             prepare(preparation_data)\n    115             self = reduction.pickle.load(from_parent)\n    116         finally:\n    117             del process.current_process()._inheriting\n--> 118     return self._bootstrap()\n        self._bootstrap = <bound method BaseProcess._bootstrap of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    119 \n    120 \n    121 def _check_not_importing_main():\n    122     if getattr(process.current_process(), '_inheriting', False):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in _bootstrap(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n    253                 # delay finalization of the old process object until after\n    254                 # _run_after_forkers() is executed\n    255                 del old_process\n    256             util.info('child process calling self.run()')\n    257             try:\n--> 258                 self.run()\n        self.run = <bound method BaseProcess.run of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    259                 exitcode = 0\n    260             finally:\n    261                 util._exit_function()\n    262         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in run(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, <sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py in worker(inqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, outqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = None\n        func = <sklearn.externals.joblib._parallel_backends.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception and func is not _helper_reraises_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py in __call__(self=<sklearn.externals.joblib._parallel_backends.SafeFunction object>, *args=(), **kwargs={})\n    345     def __init__(self, func):\n    346         self.func = func\n    347 \n    348     def __call__(self, *args, **kwargs):\n    349         try:\n--> 350             return self.func(*args, **kwargs)\n        self.func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    351         except KeyboardInterrupt:\n    352             # We capture the KeyboardInterrupt and reraise it as\n    353             # something different, as multiprocessing does not\n    354             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Mar 29 15:18:21 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...        random_state=815305645, splitter='best')>\n        X = array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nJoblibValueError                                   Fri Mar 29 15:18:23 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    735 %s\"\"\" % (this_report, exception.message)\n    736                     # Convert this to a JoblibException\n    737                     exception_type = _mk_exception(exception.etype)[0]\n    738                     exception = exception_type(report)\n    739 \n--> 740                     raise exception\n        exception = undefined\n    741 \n    742     def __call__(self, iterable):\n    743         if self._jobs:\n    744             raise ValueError('This Parallel instance is already running')\n\nJoblibValueError: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<string> in <module>()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in spawn_main(pipe_handle=2628, parent_pid=15660, tracker_fd=None)\n    100         fd = msvcrt.open_osfhandle(new_handle, os.O_RDONLY)\n    101     else:\n    102         from . import semaphore_tracker\n    103         semaphore_tracker._semaphore_tracker._fd = tracker_fd\n    104         fd = pipe_handle\n--> 105     exitcode = _main(fd)\n        exitcode = undefined\n        fd = 3\n    106     sys.exit(exitcode)\n    107 \n    108 \n    109 def _main(fd):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in _main(fd=3)\n    113             preparation_data = reduction.pickle.load(from_parent)\n    114             prepare(preparation_data)\n    115             self = reduction.pickle.load(from_parent)\n    116         finally:\n    117             del process.current_process()._inheriting\n--> 118     return self._bootstrap()\n        self._bootstrap = <bound method BaseProcess._bootstrap of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    119 \n    120 \n    121 def _check_not_importing_main():\n    122     if getattr(process.current_process(), '_inheriting', False):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in _bootstrap(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n    253                 # delay finalization of the old process object until after\n    254                 # _run_after_forkers() is executed\n    255                 del old_process\n    256             util.info('child process calling self.run()')\n    257             try:\n--> 258                 self.run()\n        self.run = <bound method BaseProcess.run of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    259                 exitcode = 0\n    260             finally:\n    261                 util._exit_function()\n    262         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in run(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, <sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py in worker(inqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, outqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = None\n        func = <sklearn.externals.joblib._parallel_backends.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception and func is not _helper_reraises_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py in __call__(self=<sklearn.externals.joblib._parallel_backends.SafeFunction object>, *args=(), **kwargs={})\n    345     def __init__(self, func):\n    346         self.func = func\n    347 \n    348     def __call__(self, *args, **kwargs):\n    349         try:\n--> 350             return self.func(*args, **kwargs)\n        self.func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    351         except KeyboardInterrupt:\n    352             # We capture the KeyboardInterrupt and reraise it as\n    353             # something different, as multiprocessing does not\n    354             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Mar 29 15:18:21 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...        random_state=815305645, splitter='best')>\n        X = array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nJoblibValueError                                   Fri Mar 29 15:18:23 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    735 %s\"\"\" % (this_report, exception.message)\n    736                     # Convert this to a JoblibException\n    737                     exception_type = _mk_exception(exception.etype)[0]\n    738                     exception = exception_type(report)\n    739 \n--> 740                     raise exception\n        exception = undefined\n    741 \n    742     def __call__(self, iterable):\n    743         if self._jobs:\n    744             raise ValueError('This Parallel instance is already running')\n\nJoblibValueError: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<string> in <module>()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in spawn_main(pipe_handle=2628, parent_pid=15660, tracker_fd=None)\n    100         fd = msvcrt.open_osfhandle(new_handle, os.O_RDONLY)\n    101     else:\n    102         from . import semaphore_tracker\n    103         semaphore_tracker._semaphore_tracker._fd = tracker_fd\n    104         fd = pipe_handle\n--> 105     exitcode = _main(fd)\n        exitcode = undefined\n        fd = 3\n    106     sys.exit(exitcode)\n    107 \n    108 \n    109 def _main(fd):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in _main(fd=3)\n    113             preparation_data = reduction.pickle.load(from_parent)\n    114             prepare(preparation_data)\n    115             self = reduction.pickle.load(from_parent)\n    116         finally:\n    117             del process.current_process()._inheriting\n--> 118     return self._bootstrap()\n        self._bootstrap = <bound method BaseProcess._bootstrap of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    119 \n    120 \n    121 def _check_not_importing_main():\n    122     if getattr(process.current_process(), '_inheriting', False):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in _bootstrap(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n    253                 # delay finalization of the old process object until after\n    254                 # _run_after_forkers() is executed\n    255                 del old_process\n    256             util.info('child process calling self.run()')\n    257             try:\n--> 258                 self.run()\n        self.run = <bound method BaseProcess.run of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    259                 exitcode = 0\n    260             finally:\n    261                 util._exit_function()\n    262         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in run(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, <sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py in worker(inqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, outqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = None\n        func = <sklearn.externals.joblib._parallel_backends.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception and func is not _helper_reraises_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py in __call__(self=<sklearn.externals.joblib._parallel_backends.SafeFunction object>, *args=(), **kwargs={})\n    345     def __init__(self, func):\n    346         self.func = func\n    347 \n    348     def __call__(self, *args, **kwargs):\n    349         try:\n--> 350             return self.func(*args, **kwargs)\n        self.func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    351         except KeyboardInterrupt:\n    352             # We capture the KeyboardInterrupt and reraise it as\n    353             # something different, as multiprocessing does not\n    354             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Mar 29 15:18:21 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...        random_state=815305645, splitter='best')>\n        X = array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-bc150bd7f799>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrid_search_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrefit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'precision_score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-469f35795da2>\u001b[0m in \u001b[0;36mgrid_search_wrapper\u001b[1;34m(refit_score)\u001b[0m\n\u001b[0;32m      7\u001b[0m     grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n\u001b[0;32m      8\u001b[0m                            cv=skf, return_train_score=True, n_jobs=-1)\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibException\u001b[0m: JoblibException\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x0000028E180A6DB0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anacon...ges\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\h...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x0000028E180A6DB0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anacon...ges\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\h...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 3, 29, 14, 17, 11, 74099, tzinfo=tzutc()), 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'session': 'B6D358B33BA14A83935DFEDC7485FFB8', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'B6D358B33BA14A83935DFEDC7485FFB8']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 3, 29, 14, 17, 11, 74099, tzinfo=tzutc()), 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'session': 'B6D358B33BA14A83935DFEDC7485FFB8', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'B6D358B33BA14A83935DFEDC7485FFB8'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 3, 29, 14, 17, 11, 74099, tzinfo=tzutc()), 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'session': 'B6D358B33BA14A83935DFEDC7485FFB8', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6EFB5FBECDF0461F81A491A6AF62F20B', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = \"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\"\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\",), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\",)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"grid_search_clf = grid_search_wrapper(refit_score='precision_score')\\n\", store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>], cell_name='<ipython-input-78-bc150bd7f799>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 28e24146d68, executio..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x0000028E2470E930, file \"<ipython-input-78-bc150bd7f799>\", line 1>\n        result = <ExecutionResult object at 28e24146d68, executio..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x0000028E2470E930, file \"<ipython-input-78-bc150bd7f799>\", line 1>, result=<ExecutionResult object at 28e24146d68, executio..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x0000028E2470E930, file \"<ipython-input-78-bc150bd7f799>\", line 1>\n        self.user_global_ns = {'C': <scipy.stats._distn_infrastructure.rv_frozen object>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...accuracy_score, precision_score, confusion_matrix', \"get_ipython().magic('matplotlib inline')\", \"weekly = pd.read_csv(filepath_or_buffer='Courier_weekly_data.csv')\", 'weekly.head(20)', \"weekly[(weekly.week==11) | (weekly.week==10) | (...ly.week==9)]['courier'].drop_duplicates().count()\", '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', 'weeks=weekly.copy()', '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', \"week['label']=week.apply(week_label, axis=1)\", \"weeks['label']=weeks.apply(week_label, axis=1)\", 'weeks=weeks[(weeks.week<8)]', 'weeks.head()', 'def max_consecutive(vector):\\n    longest = 0\\n   ...urrent = 0\\n\\n    return max(longest, current)\\n    ', 'def max_streak(series):\\n    week_vector=[0,0,0,0..._vector)\\n    return streak\\n    \\n        \\n        ', \"gold=weeks.groupby(['courier'], as_index=False ).agg({week:[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':['count', max_consecutive]})\", 'gold', 'weeks.head(20)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNN': <class 'fancyimpute.knn.KNN'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:     courier  week  feature_1  feature_2  feature...     0.8732   47.096181           1           9  , 5: 387, 12:    courier  week  feature_1  feature_2  feature_...  1  \n1      1  \n2      1  \n3      1  \n4      0  , 18:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 19:     courier  week  feature_1  feature_2  feature... \n23      1  \n24      1  \n25      1  \n26      1  , 20: 4, 21: 3, 22: 6, 25:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 28:     courier  week           \n            count m...  519341     3          2\n\n[729 rows x 3 columns], ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, ...}\n        self.user_ns = {'C': <scipy.stats._distn_infrastructure.rv_frozen object>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import pandas as pd\\nimport numpy as np\\nimport ma...accuracy_score, precision_score, confusion_matrix', \"get_ipython().magic('matplotlib inline')\", \"weekly = pd.read_csv(filepath_or_buffer='Courier_weekly_data.csv')\", 'weekly.head(20)', \"weekly[(weekly.week==11) | (weekly.week==10) | (...ly.week==9)]['courier'].drop_duplicates().count()\", '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', 'weeks=weekly.copy()', '\\ndef week_label(row):\\n    courier_set=weekly[(we...bel=1\\n    else: \\n        label=0\\n    return label', \"week['label']=week.apply(week_label, axis=1)\", \"weeks['label']=weeks.apply(week_label, axis=1)\", 'weeks=weeks[(weeks.week<8)]', 'weeks.head()', 'def max_consecutive(vector):\\n    longest = 0\\n   ...urrent = 0\\n\\n    return max(longest, current)\\n    ', 'def max_streak(series):\\n    week_vector=[0,0,0,0..._vector)\\n    return streak\\n    \\n        \\n        ', \"gold=weeks.groupby(['courier'], as_index=False ).agg({week:[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':[count, max_consecutive]})\", \"gold=weeks.groupby(['courier'], as_index=False ).agg({'week':['count', max_consecutive]})\", 'gold', 'weeks.head(20)', ...], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'KNN': <class 'fancyimpute.knn.KNN'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {4:     courier  week  feature_1  feature_2  feature...     0.8732   47.096181           1           9  , 5: 387, 12:    courier  week  feature_1  feature_2  feature_...  1  \n1      1  \n2      1  \n3      1  \n4      0  , 18:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 19:     courier  week  feature_1  feature_2  feature... \n23      1  \n24      1  \n25      1  \n26      1  , 20: 4, 21: 3, 22: 6, 25:     courier  week                \n            co...341     3               0\n\n[729 rows x 3 columns], 28:     courier  week           \n            count m...  519341     3          2\n\n[729 rows x 3 columns], ...}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<ipython-input-78-bc150bd7f799> in <module>()\n----> 1 grid_search_clf = grid_search_wrapper(refit_score='precision_score')\n\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<ipython-input-77-469f35795da2> in grid_search_wrapper(refit_score='precision_score')\n      4     prints classifier performance metrics\n      5     \"\"\"\n      6     skf = StratifiedKFold(n_splits=10)\n      7     grid_search = GridSearchCV(clf, param_grid, scoring=scorers, refit=refit_score,\n      8                            cv=skf, return_train_score=True, n_jobs=-1)\n----> 9     grid_search.fit(x_train.values, y_train.values)\n     10 \n     11     \n     12     y_pred = grid_search.predict(x_test.values)\n     13 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=StratifiedKFold(n_splits=10, ran...: make_scorer(accuracy_score)},\n       verbose=0), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method StratifiedKFold.split of Stratifie...d(n_splits=10, random_state=None, shuffle=False)>\n        X = array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y = array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64)\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nJoblibValueError                                   Fri Mar 29 15:18:23 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in retrieve(self=Parallel(n_jobs=-1))\n    735 %s\"\"\" % (this_report, exception.message)\n    736                     # Convert this to a JoblibException\n    737                     exception_type = _mk_exception(exception.etype)[0]\n    738                     exception = exception_type(report)\n    739 \n--> 740                     raise exception\n        exception = undefined\n    741 \n    742     def __call__(self, iterable):\n    743         if self._jobs:\n    744             raise ValueError('This Parallel instance is already running')\n\nJoblibValueError: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\huartesa\\Documents\\other\\jobs\\Data_Scientist_Interview\\<string> in <module>()\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in spawn_main(pipe_handle=2628, parent_pid=15660, tracker_fd=None)\n    100         fd = msvcrt.open_osfhandle(new_handle, os.O_RDONLY)\n    101     else:\n    102         from . import semaphore_tracker\n    103         semaphore_tracker._semaphore_tracker._fd = tracker_fd\n    104         fd = pipe_handle\n--> 105     exitcode = _main(fd)\n        exitcode = undefined\n        fd = 3\n    106     sys.exit(exitcode)\n    107 \n    108 \n    109 def _main(fd):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\spawn.py in _main(fd=3)\n    113             preparation_data = reduction.pickle.load(from_parent)\n    114             prepare(preparation_data)\n    115             self = reduction.pickle.load(from_parent)\n    116         finally:\n    117             del process.current_process()._inheriting\n--> 118     return self._bootstrap()\n        self._bootstrap = <bound method BaseProcess._bootstrap of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    119 \n    120 \n    121 def _check_not_importing_main():\n    122     if getattr(process.current_process(), '_inheriting', False):\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in _bootstrap(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n    253                 # delay finalization of the old process object until after\n    254                 # _run_after_forkers() is executed\n    255                 del old_process\n    256             util.info('child process calling self.run()')\n    257             try:\n--> 258                 self.run()\n        self.run = <bound method BaseProcess.run of <SpawnProcess(SpawnPoolWorker-129, started daemon)>>\n    259                 exitcode = 0\n    260             finally:\n    261                 util._exit_function()\n    262         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\process.py in run(self=<SpawnProcess(SpawnPoolWorker-129, started daemon)>)\n     88     def run(self):\n     89         '''\n     90         Method to be run in sub-process; can be overridden in sub-class\n     91         '''\n     92         if self._target:\n---> 93             self._target(*self._args, **self._kwargs)\n        self._target = <function worker>\n        self._args = (<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, <sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, None, (), None, True)\n        self._kwargs = {}\n     94 \n     95     def start(self):\n     96         '''\n     97         Start child process\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\multiprocessing\\pool.py in worker(inqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, outqueue=<sklearn.externals.joblib.pool.CustomizablePicklingQueue object>, initializer=None, initargs=(), maxtasks=None, wrap_exception=True)\n    114             util.debug('worker got sentinel -- exiting')\n    115             break\n    116 \n    117         job, i, func, args, kwds = task\n    118         try:\n--> 119             result = (True, func(*args, **kwds))\n        result = None\n        func = <sklearn.externals.joblib._parallel_backends.SafeFunction object>\n        args = ()\n        kwds = {}\n    120         except Exception as e:\n    121             if wrap_exception and func is not _helper_reraises_exception:\n    122                 e = ExceptionWithTraceback(e, e.__traceback__)\n    123             result = (False, e)\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py in __call__(self=<sklearn.externals.joblib._parallel_backends.SafeFunction object>, *args=(), **kwargs={})\n    345     def __init__(self, func):\n    346         self.func = func\n    347 \n    348     def __call__(self, *args, **kwargs):\n    349         try:\n--> 350             return self.func(*args, **kwargs)\n        self.func = <sklearn.externals.joblib.parallel.BatchedCalls object>\n        args = ()\n        kwargs = {}\n    351         except KeyboardInterrupt:\n    352             # We capture the KeyboardInterrupt and reraise it as\n    353             # something different, as multiprocessing does not\n    354             # interrupt processing for a KeyboardInterrupt\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), {'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), 0, {'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 8.00000000e+00,  8.00000000e+00,  2.500...00000000e+00,  2.45666670e+03,  3.07083338e+02]]), y=array([0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,...1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1], dtype=int64), scorer={'accuracy_score': make_scorer(accuracy_score), 'precision_score': make_scorer(precision_score), 'recall_score': make_scorer(recall_score)}, train=array([ 49,  50,  53,  57,  60,  61,  62,  63,  ...    537, 538, 539, 540, 541, 542, 543, 544, 545]), test=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 1..., 46, 47, 48, 51, 52,\n       54, 55, 56, 58, 59]), verbose=0, parameters={'max_depth': 3, 'max_features': 20, 'min_samples_split': 3, 'n_estimators': 100}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas...e=None, verbose=0,\n            warm_start=False)>\n        X_train = array([[ 2.00000000e+00,  1.00000000e+00, -5.000...00000000e+00,  2.45666670e+03,  3.07083338e+02]])\n        y_train = array([0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0,..., 0, 0, 1,\n       0, 1, 1, 0, 0, 1], dtype=int64)\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in fit(self=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None)\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 99\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Mar 29 15:18:21 2019\nPID: 19924Python 3.6.3: C:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\python.exe\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _parallel_build_trees>, (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100), {'class_weight': None, 'verbose': 0})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _parallel_build_trees>\n        args = (DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), None, 0, 100)\n        kwargs = {'class_weight': None, 'verbose': 0}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py in _parallel_build_trees(tree=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), forest=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=None, tree_idx=0, n_trees=100, verbose=0, class_weight=None)\n    116                 warnings.simplefilter('ignore', DeprecationWarning)\n    117                 curr_sample_weight *= compute_sample_weight('auto', y, indices)\n    118         elif class_weight == 'balanced_subsample':\n    119             curr_sample_weight *= compute_sample_weight('balanced', y, indices)\n    120 \n--> 121         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n        tree.fit = <bound method DecisionTreeClassifier.fit of Deci...        random_state=815305645, splitter='best')>\n        X = array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32)\n        y = array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]])\n        sample_weight = None\n        curr_sample_weight = array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.])\n    122     else:\n    123         tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n    124 \n    125     return tree\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    785 \n    786         super(DecisionTreeClassifier, self).fit(\n    787             X, y,\n    788             sample_weight=sample_weight,\n    789             check_input=check_input,\n--> 790             X_idx_sorted=X_idx_sorted)\n        X_idx_sorted = None\n    791         return self\n    792 \n    793     def predict_proba(self, X, check_input=True):\n    794         \"\"\"Predict class probabilities of the input samples X.\n\n...........................................................................\nC:\\Users\\huartesa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py in fit(self=DecisionTreeClassifier(class_weight=None, criter...         random_state=815305645, splitter='best'), X=array([[ 2.0000000e+00,  1.0000000e+00, -5.00000...  2.4566667e+03,  3.0708334e+02]], dtype=float32), y=array([[0.],\n       [0.],\n       [0.],\n       [0...    [1.],\n       [0.],\n       [0.],\n       [1.]]), sample_weight=array([1., 2., 1., 3., 1., 0., 2., 2., 0., 2., 2... 1., 2., 3., 4., 1., 0., 0., 2., 1., 1., 1., 1.]), check_input=False, X_idx_sorted=None)\n    237         if not 0 <= self.min_weight_fraction_leaf <= 0.5:\n    238             raise ValueError(\"min_weight_fraction_leaf must in [0, 0.5]\")\n    239         if max_depth <= 0:\n    240             raise ValueError(\"max_depth must be greater than zero. \")\n    241         if not (0 < max_features <= self.n_features_):\n--> 242             raise ValueError(\"max_features must be in (0, n_features]\")\n    243         if not isinstance(max_leaf_nodes, (numbers.Integral, np.integer)):\n    244             raise ValueError(\"max_leaf_nodes must be integral number but was \"\n    245                              \"%r\" % max_leaf_nodes)\n    246         if -1 < max_leaf_nodes < 2:\n\nValueError: max_features must be in (0, n_features]\n___________________________________________________________________________\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "grid_search_clf = grid_search_wrapper(refit_score='precision_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of random forest classifier on test set: 0.76\n"
     ]
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier( max_depth=25, max_features=3, min_samples_split=10, n_estimators=300)\n",
    "randomForest.fit(x_train, y_train)\n",
    "print('Accuracy of random forest classifier on test set: {:.2f}'.format(randomForest.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72, 21],\n",
       "       [23, 67]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_pred = randomForest.predict(x_test)\n",
    "cf_mt = confusion_matrix(y_test, test_y_pred)\n",
    "cf_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7595628415300546\n",
      "Accuracy: 0.792 (0.081)\n",
      "Logloss: -0.487 (0.137)\n",
      "AUC: 0.853 (0.097)\n",
      "[[71 22]\n",
      " [23 67]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,15,'Predicted label')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEoCAYAAACzVD1FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XeYFFXWx/HvGQYkKEoWxAi6uKJi\nQGEVAwaCCAiYA666GFZfc1oxu2bEXUVdjJgFswiiogIKBhBEkqAIogRRQSUIDHPeP6oGm65mpgem\n0/Tvs08905Vun8bZPnND3WvujoiISKyCTAcgIiLZR8lBREQilBxERCRCyUFERCKUHEREJELJQURE\nIpQcREQkQslBREQilBxERCRCyUFERCIKMx1ANlrz02zNKSLrqdGkXaZDkCxVtPoH25T7y/N9U7X+\nTqW+l5n9BXgh5tBOwHXAk+HxHYA5wHHuvqS0slRzEBHJpOK1yW9lcPev3L2Vu7cC9gFWAK8AVwEj\n3X1nYGS4XyolBxGRTPLi5LfyOQz4xt3nAt2AQeHxQUD3sm5Ws5KISAb52qJUFX0C8Fz4upG7LwBw\n9wVm1rCsm1VzEBHJpOLipDcz62Nm42O2PomKNLNqQFdgyMaGpZqDiEgmlaO5yN0HAgOTuLQT8Lm7\nLwr3F5lZ47DW0Bj4sawCVHMQEcmkCuyQjnEifzYpAbwO9A5f9wZeK6sA1RxERDKp/B3NpTKzmsAR\nwNkxh28HBpvZmcB3wLFllaPkICKSScUVmxzcfQVQL+7YzwSjl5Km5CAikkEpHK20SZQcREQyqYKb\nlSqKkoOISCaVr6M5bZQcREQySTUHERGJqOAO6Yqi5CAikkmqOYiISDxfuybTISSk5CAikkmqOYiI\nSIT6HEREJEI1BxERidBzDiIiEqHpM0REJELNSiIiEqEOaRERiVByEBGReO7qkBYRkXiqOYiISIRG\nK4mISIRGK4mISISalUREJEI1BxERiVDNQUREItQhLSIiEao5iIhIhPocREQkQjUHERGJUM1BREQi\nVHMQEZGItZp4T0RE4qnmICIiEUoOIiISoQ5pERGJUM1BREQi3DMdQUJKDiIimVSkuZVERCRelvY5\nFGQ6ABGRfObFnvSWDDPbysxeNLMZZjbdzNqaWV0ze8fMZoU/65RVjmoOeejVN9+h7633lHpNQUEB\nk8e8CcCaoiKef3koX82azfSZ3/DNnO8oKirihisvpFfXjukIWVKsbt06dO/Wkc6dD6Plbi3YZput\nWb16DVOmzOCJQS/wxKAX8Ji28ebNd+SY7p048ohDaN58Rxo1qs+SJb/yyaef89//PsIHo8Zm8NPk\nmIrvkP4P8Ja79zKzakBN4F/ASHe/3cyuAq4CriytECWHPNRi550494yTE577/IspfDLhCw5ss++6\nYytX/sEd//kfAPXq1qF+vTosXLQ4LbFKevTq2YUHBtzO/PkL+WDUWOa9/AMNGzbgmO6deHhgPzp2\nbM/xJ/RZd/2NN1zO8cd1Y+q0rxj+1nssWbKEXXZpxtFdjqTr0R246OJruX/AYxn8RDmkApuVzKw2\ncBBwOoC7rwZWm1k34JDwskHAByg5SLwWuzSjxS7NEp47uc/FABzbtdO6YzWqb8aDd99Ei52b0aB+\nXQY8+jQPPvZMWmKV9Jg1azbdjzmdN4e9u14Noe+1tzPuozfp2eMojjmmM6+8MgyAESM+4K67BzBp\n0tT1yjmoXRveGv4cd9zelxdfGsrChT+m9XPkpKLkp88wsz5An5hDA919YMz+TsBi4HEz2xOYAFwI\nNHL3BQDuvsDMGpb1XjnT52BmhWbmZtY907FUVrNmz+GLqTNo1KAeB/2t9brjVatWpV3b1jSoXzeD\n0Ukqvf/BRwx98531EgPAokWLGfjwUwAcfFDbdceffGpwJDEAjB7zMaNGjWOzzTajbdt9I+clgeLi\npDd3H+ju+8ZsA+NKKwT2Bh50972A5QRNSOWW1uRgZk+EX/DxW6t0xiGJDXk1+KvwmC4dqFKlSoaj\nkWyxZs0aANYm+RfumjXB0MyiLB2imXXck9/K9j3wvbt/Eu6/SJAsFplZY4DwZ5lVukzUHN4FGsdt\nUzIQh8T4Y9Uqhr79PgUFBfQ8Wp3MEqhSpQqnnNILgBFvv1/m9dtttw3t2x/A8uUrGDPmkzKvF8pV\ncyiLuy8E5pnZX8JDhwHTgNeB3uGx3sBrZZWVieSwyt0Xxm1FZtbZzD40s6Vm9ouZDY/5gBEWuMHM\n5prZKjNbYGaPx5wvMLOrzWy2ma00sy/N7MT0fMTcM2LkaH77fRkHttmXxo0aZDocyRK3/ftf7N5y\nV4YNG8nb74wq9dpq1arx1KD7qV69Ojfd3I+lS39NU5Q5rtiT35JzAfCMmU0GWgG3ArcDR5jZLOCI\ncL9U2dQhXQu4B/iSYOjVdcAbZrabu69JcP1xwEXAicBUoBHQOub8bUBX4FxgJnAA8KiZLXH3t1L2\nKXLUkNeDf5LjunUq40rJF+f/8wwuueQcps+YRe+//1+p1xYUFDDoif9ywAH78cLg1+h3z0NpirIS\nqOCH4Nx9EpCow+ew8pSTieTQ0cyWxeyPcfdO7j4k9iIz+zuwFNgH+DhBOdsD84F33L0I+A74LLx3\nC4Ie+kPdfVx4/bdm1gY4D4gkh9hRAA/0u4WzTsufSsY3385l0pfTaNSwPu3ati77Bqn0zj2nN/f2\nv5mp077iyA7Hs2TJ0g1eW1BQwJOD7uPYXkczeMjrnNb7gjRGmvu8HKOV0ikTyWE06w/FWglgZjsD\nNwH7A/UJmrwM2I7EyeEFgurTt2Y2guAL//VwXG9LYDPgHTOLvacq8HWioMJe/4EAa36anZ0zYaXI\n4NeGA9BDHdEC/N8FZ3FPvxv5csp0juxwPIsX/7zBa6tUqcLTTw3g2F5H8+xzL3P63y+kOEtnGc1a\nyTcXpVUmksMKd0/0Bf0m8C3wD4IaQTFBR0q1RIW4+1wz2wU4nKC61B+41sza8mdfylHAD3G3rt7k\nT1CJrFq1mjfeGklBQQE9unTIdDiSYZdfdh633XoNEydNoWOnE/j55yUbvLZq1ao8/9xDdOvakSef\nGsKZZ10cGQorScjSuZWyos/BzBoBOwNnuvuY8Nh+lNFh7u4rgTcI+ibuIhjG1YbgwY/VwHbuXnov\nWp4b8f4Yfvt9GQcfsJ86ovPcNf+6iBtvuJzxE76gU+eTSm1KqlatGi8OfoTOnQ/j0cee5Zxzr1Bi\n2FiqOZTqJ+AXoI+ZLQCaAncR1B4SMrMzwpefEjzocRKwBvja3X81s/5AfzOrAowBagNtgdXu/kjK\nPkmOeTFsUop9IjqRR54azLdz5wEwY9ZsAF4d9g4TJwcPQu21x26aZymHnXrqsdx4w+UUFRXx0Yef\ncsH5Z0SumTPne558ajAADwy4nc6dD2Px4p+ZP38h1/a9OHL9qFHjGDV6XOS4xMnSZrisSA7uvtbM\njieYMGoKMAu4GBhaym1LgSsIRjgVEjRBdXf378LzVwMLCeYPGQj8CkwE7kjFZ8hF38z5js8nT02q\nI/rDT8YzfuKX6x2b9OU0Jn05bd2+kkPu2nGHbQEoLCzkwgv/kfCaUaPGrksOJdc3aFCPa/tekvD6\nm+in5JCMLK05mKqCUfnWIS1lq9GkXaZDkCxVtPoHK/uqDVt+zbFJf9/U+veQTXqv8siKmoOISL5y\nNSuJiEhEljYrKTmIiGSSkoOIiEToOQcREYnnRUoOIiIST81KIiISodFKIiISoZqDiIhEKDmIiEi8\nbJ2lQslBRCSTNFpJRETiuZqVREQkQslBREQisrNVacPJwcw6J1uIuw+rmHBERPJLLjYrlbbQTiwH\ntCq9iMjGyMHkUCNtUYiI5CkvyrHk4O6r0hmIiEheytI+h4JkLzSz9mb2oplNNLOm4bHTzezg1IUn\nIlK5ebEnvaVTUsnBzI4F3gAWAy2AauGpmsBVqQlNRCQPFJdjS6Nkaw7XAOe4+7lAUczxscBeFR6V\niEie8OLkt3RK9jmHXYDRCY7/BmxVceGIiOQXLyr7mkxItuawEGie4PgBwOyKC0dEJM9kabNSsjWH\nR4F7zex0gucaGplZa+Au4PYUxSYiUull6RLSSSeHW4G6BH0MVYGPCPoe/uPu96YoNhGRSi+nk4MH\nE45famY3AbsTNEd96e5LUhmciEhll9PJIcZygv4HgN8rOBYRkfzjlukIEkr2OYeqZnY7sBT4KtyW\nmtkdZlat9LtFRGRDioss6S2dkq053A90BS4ExoXH2gI3EwxlPbviQxMRqfwqulnJzOYQtOysBYrc\nfV8zqwu8AOwAzAGOK6tbINnkcAJwvLu/FXNsmpnNB55HyUFEZKN4apqVDnX3n2L2rwJGuvvtZnZV\nuH9laQUk+5zDH8DcBMfnAKuTLENEROKk6QnpbsCg8PUgoHtZNySbHB4E/hXbv2BmVQmyz4PlDFJE\nREJebElvyRYJvG1mE8ysT3iskbsvAAh/NiyrkNJWghscd6gjcKSZTQz3WxGs+TAi2YhFRGR9Xo7J\nVsMv+z4xhwa6+8C4yw5w9/lm1hB4x8xmbExcpfU5rI3bfzNu//2NeUMREflTcVHSKycQJoL4ZBB/\nzfzw549m9gqwH7DIzBq7+wIzawz8WNZ7lbbYz4lJRywiIhulPDWHsphZLaDA3X8PXx8J3AS8DvQm\nmO6oN/BaWWWV9yE4ERGpQOXoS0hGI+AVM4Pg+/1Zd3/LzD4DBpvZmcB3wLFlFZR0cjCzE4ETge34\nc7EfANz9r8nHLiIiJSpyKKu7zwb2THD8Z+Cw8pSV7BPSFwEPAd8QrAT3HjAPaAK8WJ43FBGRP+X6\nYj/nAn3c/QUzOwu4x91nhxPxNUhdeCIildva4uQ7pNMp2ai2BT4OX68EtghfPwUcV9FBiYjkixQ8\n51Ahkk0OiwjWc4CgM2O/8PX2QHZOKSgikgPck9/SKdlmpfeBLsBEgkev7zWzHsD+JDEkSkREEkt3\njSBZySaHc0qudff7zOw3gvWjRwL3pSg2EZFKrzhL13NIdiW41cRMsOfug/hzEicREdlIKZqVdZOV\nNrdS0s8uuPu0iglHRCS/rM3BZqUpBLP7JWLhuZKfVSo4LhGRvJBzNQdg17RFISKSp9I9CilZpU28\n91U6A8kmtbc9NNMhSJZZ9rGWLZHUyOkOaRERSY1cbFYSEZEUW6vkICIi8dSsJCIiEdnarFSu6QDN\nbHMz29PMqqYqIBGRfFJcji2dkl3PoZaZPQn8BkwgmKUVM7vfzK5JYXwiIpWaY0lv6ZRszeE2gkV+\n/gb8EXP8bZJYbk5ERBIr9uS3dEq2z6EbcJy7f2JmsSFOA3aq+LBERPLD2vK17qdNssmhAfBjguO1\nKjAWEZG8k+6+hGQlm7ImAJ1j9ktqD2cA4yo0IhGRPJKtfQ7J1hyuAYaZWYvwnn+a2W7AIcDBKYpN\nRKTSy+mag7uPJkgCDYEfgB7AcuAAd/80deGJiFRu2TqUNemH4Nx9AnB8CmMREck76W4uSlZSycHM\napZ23t1XVEw4IiL5pchyODkAy9jwwj+gxX5ERDZKli7nkHRy6BS3XxXYCzgLuLZCIxIRySPZ2iGd\nVHJw9xEJDg81s5nAKcCTFRqViEieKM7SZqVNfTRvPNC+IgIREclHXo4tnTZ6ym4zqwb8k2Boq4iI\nbISi7Kw4JD1aaTHrJy4DtgJWA6elIC4RkbxQnMtDWYG+cfvFwGJgrLsnmnNJRESSkLOjlcysEFgD\nDHP3hakPSUQkfxRnZ8Wh7A5pdy8C7gc2S304IiL5JVunz0h2tNKnwJ6pDEREJB/l+mil+4F+ZtaE\nYPru5bEn3X1aRQcmIpIPUjFaycyqEDxq8IO7dzGzHYHngbrA58Cp7r66tDKSTQ6Dw58PhD9LkpiF\nrzV9hojIRkhRc9GFwHSgdrh/B9Df3Z83s4eAM4EHSysg2eSw60aHKCIiG+QVXHMws6bAUcC/gUvM\nzAgeVj4pvGQQcAObkhzM7DHgQnf/alMDFhGRqBTUHO4FrgC2CPfrAUvDwUUA3wPblFVIWR3SvYEa\nGxuhiIiUrjyjlcysj5mNj9n6xJZlZl2AH8P1d9YdTvC2ZfZvl9WslKUjcEVEKofyjEJy94HAwFIu\nOQDoamadgeoEfQ73AluZWWFYe2gKzC/rvZIZypqtD/CJiOS8Ikt+K4u7X+3uTd19B+AE4D13Pxl4\nH+gVXtYbeK2sspJJDgvNbG1pWxJliIhIAml6CO5Kgs7prwn6IB4t64ZkRiv1AZZuWlwiIpJIqppm\n3P0D4IPw9Wxgv/Lcn0xyeEOT64mIpEa2zq1UVnJQf4OISArl6jKhWZrTREQqh7VZ+jd4qcnB3Td1\nGVERESlFrtYcREQkhbKz3qDkICKSUao5iIhIRK6OVhIRkRQqztKGJSUHEZEMytYpJpQcREQySDUH\nERGJyM7UoOQgIpJRGq0kWaNu3a3o2rUjnTodym67taBJk61ZvXo1U6d+xZNPDuHJJwfj/uffM02b\nNuayy/7J3nu3ZNttt6FOnS355ZelzJ49l0GDBvPcc69QVFRUyjtKLvl8xmyeHj6GL2bO5ddlK9hy\n85o033ZrTunUjnZ7BSsGX/vg87w+ekKp5ey3W3Me7nt2OkLOaWpWkqzRo8dR3HffrSxYsIhRo8Yx\nb94PNGzYgG7dOvDQQ3fSocMhnHTSueuu33HH7TnhhG589tkkJk16myVLllK3bh06dDiEgQPv5uST\ne3LUUSezdm22dq1Jsga+/C4Dhoygzha1aLf3rjTYqjZLf1/OjDk/MH7aN+uSw6H7tqRJg7oJyxg6\nZgLf//gLB7b6SzpDz1nZmRqUHPLSrFnf0rPnGQwf/t56NYTrr7+TMWNe45hjOtO9eydefXU4AB9/\nPIHGjfdY71qAwsJChg59moMPbkv37h156aU30/o5pGK9/fEXDBgygjYtd+aeS06jVo3q651fU/Rn\n8m/fuiXtW7eMlPHb8pU88cYHVC2sQteDWqc85sogW+dW0txJeWjUqLEMGzYy8mW/aNFiHn74GQAO\nOqjNuuNr1qyJXAtQVFTEG2+MAKBZsx1TGLGkWnFxMfc+N4zqm1XltgtOiiQGgKqFVcosZ+iYCfyx\neg2Htd6dOrVrpSLUSidNi/2UW8ZqDmZWVroc5O6npyMW+VNJ30EyfQgFBQV06HAoAFOmTE9pXJJa\nk2bO5Ycff+GI/fegdq0ajP58Ol9/v5DNqhbSstm27LnLDkmV8/J7nwDQ87D9Uxht5aI+h6jGMa+7\nAA/HHVuZ6CYzq+rua1IZWL6qUqUKJ53UA4C33x4VOV+vXh3OOac3Zkb9+vU47LADad58R55//lWG\nDRuZ7nClAk2dPQ+AultuzglX38useQvXO79Pi524++JTqVt78w2W8cXMOcyat5DtGzdgv92apzTe\nyiQ7U0MGk4O7r/vtM7Ol8cfC4y2A6cBxwPnA/sD5ZlYI3OLu9WOu7QgMB7Zw92XhsYOAW4F9gJ+A\nV4GrS87L+m655SpatmzB8OHv8e67oyPn69WrS9++F6/bLy4upn///3HddXemM0xJgV9+Df4v8eK7\nH7NNw7oMvKYPuzffjvmLl9Dv6TcYO3kml9/7FI9ed+4Gy3ixpNbQvlyrUea9bK055Eqfw+1Af2BX\nYFgyN5jZPgTJYjCwO0GCaQs8lKIYc9p5553ORRf1YcaMrznzzIsSXjNz5jfUqLE9tWrtyC67tOWK\nK27ijDNO5N13h1CnzpZpjlgqUnFx0KLt7tx90ans33JnalbfjObbbk3/S0+nUd0tGT99Nl/MnJPw\n/t9XrOTtj79QR/RGWIsnvaVTriSHe9z9VXf/1t3nJ3nPlcAT7v5fd//a3ccR1D5ONrPaqQs195x9\n9mn063cj06bNpGPHE1iy5NdSry8uLmbevPkMGPA4F1zwL/bff2+uu+7SNEUrqVC7Vk0Amjaqx1+2\nb7LeuerVqvK3PYNhqVO+mZfw/jc//Jw/VqkjemOoQ3rTjN+Ie/YBmppZ75hjJZPjNgMmxl5sZn2A\nPgCFhXUpLNxw22plcv75Z3DXXdczZcoMOnc+icWLfy7X/SNGfABAu3ZtSr9Qstr2TRoAsEXN6Cgl\ngNq1agDwx+rE3X0lHdG9DtfvQXl5ljYr5UpyWB63X0x0feuqcfsFwADggQTlRf78cfeBwECAGjW2\nz87/WhXs0kvP4ZZbrmbSpKl06XIyP/+8pNxlNGmyNZDc6CbJXvu02InCKgV8t/An1hQVUbVw/a+G\nr8MO6kQPvk3++ju+mruA7Rs3oPVfm6Ul3sokW6fPyJVmpXiLga3MLPbPnFZx13wO7BY2KcVvq9IX\nana66qr/45ZbrmbChMl07nxiqYmhdetW1Egw7r1WrZrcfff1ALz11vspi1VSr07tWhzZZk9+X/EH\n/3vp3fXOjZs8k7GTZ7JFzeocsGf0qeeXRn4MQM/2Gr66MYrdk97SKVdqDvHGAquB28xsAEET0j/i\nrrkVGGtm/wUeI6h97Ap0cPd/pjPYbHPyyT25/vpLKSoqYuzYTznvvL9Hrpk793uefvpFAC6//Dza\ntWvDmDGf8P3381mxYiVNmzbhyCMPoU6dLRk3bjx33TUg3R9DKthlp3ZlyjfzePjVkUyYMZuWzbZl\nwU9Lee+zKVQpMK77R691zUsllq34gxHjSjqi981Q5LktW5spcjI5uPsiMzsNuA04B3gPuA54POaa\nCWZ2MHAz8GF4eDYwJM3hZp0ddtgWCKa/uOCCsxJeM3r0uHXJ4bHHnmf58pXss88eHHRQG2rWrMGS\nJb8yceKXvPTSUAYNGqx5lSqBeltuztM3X8DAV97lvc+mMHnWd9SqsRnt9mrBmd3as8fO20fuGfbR\n56xctZqObVupI3ojrc3ShiVLNC1CvsuXPgdJ3pKP7st0CJKlqu/ddZNWgT5+++5Jf9+8MPfVtK04\nnZM1BxGRyiJbH4JTchARySANZRURkYjs7HFQchARyai1np3pQclBRCSDsjM1KDmIiGSU+hxERCRC\no5VERCQiW581U3IQEcmgbO1zyNWJ90REKoW1FCe9lcXMqpvZp2b2hZlNNbMbw+M7mtknZjbLzF4w\ns2pllaXkICKSQe6e9JaEVUB7d9+TYKbqjmbWBrgD6O/uOwNLgDPLKkjJQUQkg4rxpLeyeGBZuFs1\n3BxoD7wYHh8EdC+rLCUHEZEM8nL8LxlmVsXMJgE/Au8A3wBL3b1kRa7vgW3KKkfJQUQkg8qz2I+Z\n9TGz8TFbn/jy3H2tu7cCmgL7EaxjE7msrLg0WklEJIPKM5A1djnjJK5damYfAG0IVs4sDGsPTYH5\nZd2vmoOISAYVUZz0VhYza2BmW4WvawCHA9OB94Fe4WW9gdfKKks1BxGRDKrgh+AaA4PMrArBH/+D\n3X2omU0DnjezW4CJwKNlFaTkICKSQRU5fYa7Twb2SnB8NkH/Q9KUHEREMkgT74mISITmVhIRkQgt\n9iMiIhGasltERCLU5yAiIhHF6nMQEZF4qjmIiEiEag4iIhKh0UoiIhKhZiUREYlQs5KIiESo5iAi\nIhGuPgcREYmnJ6RFRCRCo5VERCRCs7KKiEiERiuJiEiERiuJiEiEmpVERCRCHdIiIhKhPgcREYlQ\ns5KIiEToITgREYlQzUFERCLU5yAiIhEarSQiIhFqVhIRkQg9IS0iIhGqOYiISES2JgfL1sAkO5hZ\nH3cfmOk4JLvo96LyK8h0AJL1+mQ6AMlK+r2o5JQcREQkQslBREQilBykLGpXlkT0e1HJqUNaREQi\nVHMQEZEIJQcREYlQchARkQglBxERiVByEBGRCM2tJJLnzMzc3c1sa2AtUMvd52Q4LMkw1RzyiJlZ\n+LOWmdU2s8L4c5JfYhJDV+Bl4H3gHTPra2bVMxyeZJCSQ56I+RI4GngOmAgMNLMLAFwPvOSl8Hei\nI/AC8DRwHPA/4CbgwEzGJpml5JAnwi+BLgRfAh8CFwEG3GNm7TIanGRMWGPsCdzl7g8Ay4FzgIHu\n/m5Gg5OMUnLIAxbYEjgXuM7d7wTGAB2AAe4+JqMBSiZVA9oAs8LfkY+AkQS/K5jZuWamGkQeUnLI\nA2GT0SqgCTDWzLYFpgBD3f0iADPrZmb7ZDBMSYP4viV3X0WQDA4FpgJvAOeGNc0aBInj4Nj+KckP\nSg6VVIIO5i2BIqAtQafjcILmA8ysMXAMsLM6piu38Eu/vZm9HHN4BnAs8B1ws7sXh8mgL3AQ8Ly7\nF2UgXMkgJYdKKKbz+WAzu9zMqrj7IoL+hruAr939H+5eHN5yPsFfiB+rYzovVAe6mNlgAHd/COgP\nNAUeN7NBBIMWzgF6uPs3GYtUMkazslYyMYmhJ/AQwQiUQe4+ycxqA/8GzgPuCG+pDxwPHOzukzIS\ntKRcye9F+LoKcDjwDPCRu3cLj/cGWobbp8Cz7v5VhkKWDFNyqITC0UdvApe6+8MJzp9PMGSxgKBJ\nob+7T01vlJJOZraLu8+M2a8CHEHwx8OH7t49Y8FJVlJyyHFmdgbwpbt/Fu4bwRj15u5+opnVAfYH\negPbALe7+zAz29LdfzWzqu6+JmMfQFLOzLYDxhPUBC6KOV4V6AIMBh53d60LLetoBEIOM7NqwJ3A\nD2GTwBdhk9JvwCFm1gM4DagKrAQWA8+Z2U7u/nNYjDoaK6GY5sX9gK2AfsCFZrbS3a8GcPc1Zjaa\noPZ4lpnVcPdTMxi2ZBF1SOeo8P/8q4FmBGPVHwX2Ck+/A7wbHvsV6OfuvYArgW8JRi4BejK6sgmf\naSlJDB0JnltYBTwJ3E2QBG6PueV3gociewE3pj1gyVpqVsph4SikteHDS58By4DT3X1yeH57d58b\nc/2dwGHA4e6+JCNBS8qYWbXwDwbMrB5BU2IVd78r7lhfgpFrTwE9CB6GbO/uizMSuGQlJYccZ2aF\n7l4UJojxBAniDGBSzOiU/YFTgROBwzQqqfIxs92BC4ErgEbAl8B84EZ3fzTmujoE/Qz3ENQajGC4\n6sS0By1ZTc1KOSj2QbWSh5Pc/VdgX2Bz4BGgVXjtzgQPuP0VOESJofIxsz0JJlL8zt1/Ab4i6GNo\nCuxgZgUlvzPuvsTdnwKaA10hgoLRAAAHw0lEQVSB1koMkohqDjkmpj25LbA3wZQYjxN8MayOqUH8\nRtDE9GU4Xcby8ItDKhEz+yswAbjD3W+IOV4I3EowweKp7v5CzLmCmAcgRRJScsghMYmhB8G0ylOB\nzYC/EDQpDHf3n8IE8THBk7BHu/uUjAUtKWNmLQmmQvnN3ZuFx9YNTQ5rC/2AfwKnuPuQjAUrOUfN\nSlnMzArCn5vDunlxDgQeBK5090MInnQtGarYy8zqhk1MfwN+IZiCWSqZsCnpU4JaQ10zewTWDU8t\nDF87cCnwAMG0GKdkKl7JPUoOWaqk6h/OlDrZzHYOH1rai2Cu/cfMbEeC2sO9wEsEQxWPMbOG4Wik\nfd3924x9CEkJM9uXYHTane7ekWBurBNjEkRRXIK4BHgWuNvMtshQ2JJj1KyUhWISw54E6y486u4X\nh+f2JVjndybBFBmz3P0fZtYA+JqgKelsgnHtrucYKh8zOwjo6e4Xhvs1CQYdDASec/ezwuOFJQMW\nwiamhuEEjCJl0hPSWSYmMewBjAXudfdrYi6ZFP5luAfBw2yPhce3IlgDeBnB7KrqcKyk3H00MBrW\n9UOtMLOXwtMDzQx3P6ukBuHuReEfCUoMkjQlhywTJoZtCZ5wfjM2MZjZZUAzC9Z9bgLsClQLZ1s9\nGdga6Kq5kvJHSc3Q3f+ISxBr3f1s1zoMspGUHLJTFWAOsLmZtXP3MWZ2FXAVQXNCEfCWmb0DvAdM\nI5hU73AlhvwVkyCKgWfMbJW7/1+m45LcpD6HLGVmzYH7CfoX5hM8sHSqu79dMm1GeN0ZwBpgrGtR\nFgEsWN6zC8FsvTMyHY/kJiWHLBY+3TwAOBC4zt3vDo8bUFCSIEREKpqSQ5Yzs2YE49QB/h12Rq63\nspeISEXTcw5ZLmwqOh9w4LrwIThNtS0iKaXkkAPcfRZwAfAH0D+cV0lEJGWUHHJEmCAuA+YCP2Q4\nHBGp5NTnkGNiF3QREUkVJQcREYlQs5KIiEQoOYiISISSg4iIRCg5iIhIhJKDVBpmNsXMbojZnxPO\nZJvuOPY1MzezHUq55gMzu78cZR4Slll/E2N7wsyGbkoZkh+UHCRlwi8iD7c1ZjbbzO42s1ppCqE1\nf049UiozO93MlqU4HpGcoSm7JdXeBU4FqgLtgEeAWsC5iS42s6oVNe24uy+uiHJE8pFqDpJqq9x9\nobvPc/dngWeA7rBeU0lnM/vUzFYDHcJzR5vZBDP7w8y+NbN/m1m1kkLNrKGZvWZmK81sbjh1+Xri\nm5XMrLaZPWhmC8Jyp5vZ8WZ2CPA4UCumpnNDeE81M7vDzL43s+Vm9pmZdYh7n45mNiMscwywS3n/\nkczslLDs383sRzMbYmbbJLi0jZlNCt9rQrjGeGw5fzOzUWa2wsx+CD9v7fLGI6LkIOm2kqAWEesO\noC/QAvgk/PJ9hmA9i92AM4BewK0x9zwBNAcOJ0g2pwE7bOhNw2nOhwMHA38H/gpcAqwmWI71ImAF\n0Djc7g5vfTy85yRgd2AQ8Ea4vjfhqn2vAu8ArYD7gDuT/ceIUQ24HtiTYC2G+sBzCa67G7gS2BeY\nDbwZriGNme0OvA28HpbTI4zpsQTliJTO3bVpS8lG8AU+NGZ/P+An4IVw/xCC2WZ7xt03Grg27lh3\ngvWxjeAvcwcOiDm/PcHCSDfEHJsDXBa+PoJghbRdNxDr6cCyuGPNwnu2izv+KvBA+PpWYCbhbAPh\nsb5hfDuU8m/zAXB/KedbhGU0jfu3Ojnmms2BpcBZ4f6TwKNx5bQK72uY6L+JNm0b2tTnIKnWMezo\nLSSoMbxGMMNsrPFx+/sA+5nZlTHHCoAaBOtk70rwpf1pyUl3n2tm80uJYy9ggbtPL0fsexMko2lB\nxWOdzQiWZyWM5WN3j52HZlw53gMAM9uboObQCqgbvi/AdsD3icp292Vm9iVBLQiCf7fmZnZ8bNHh\nz2bAj+WNS/KXkoOk2migD8FSpvM9cWfz8rj9AuBGYEiCaxfz5xdeeWzMPQUEf3W3Jog/1spNKHc9\n4eitEfzZef8jQbPSGILmpmQVEHT4909wTjP5SrkoOUiqrXD3r8t5z+dAiw3dZ2bTCb4IWxP0F2Bm\n2wFNyiizsZntuoHaw2qgStyxiQRf/lu7+/sbKHca0DNuZb42pcSRSAuCZPAvd/8WwMx6bODaNgR9\nDSVJpSVBcxIEn3G3jfj3FolQh7Rko5uAk8zsJjNraWYtzKyXmd0J4O5fAW8B/zOztmbWiqAtfeWG\ni2Qk8Anwkpl1MLMdzewIM+senp8DVA+P1Tezmu4+k6Bj/Inw/XcKH3C7LObL+yGCjvB7zewvZtYL\nOKecn/c7YBVwfvgeRwE3b+DavmGMuxF0NK8Gng3P3UHQHPeQme1lZs3NrIuZ/a+c8YgoOUj2cfcR\nwFHAoQT9Cp8CVxF8iZY4HfiWoO3/DYIvyDmllFkMdAI+Ap4GpgP/IWy2cfexBF/0zxE0XV0R3vp3\nghFLdwIzgKHAQQSLLuHu3xGMCuoIfAFcHMZans+7GOhN0Ok+jaDv4ZINXH4V0I+glrAz0MXdl4fl\nTA5j2wEYFcZzG7CoPPGIgNZzEBGRBFRzEBGRCCUHERGJUHIQEZEIJQcREYlQchARkQglBxERiVBy\nEBGRCCUHERGJUHIQEZGI/wdW1QCmyMc39QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e2412fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(randomForest.score(x_test, y_test))\n",
    "seed = 7\n",
    "k_fold = KFold(n_splits=10, random_state=seed)\n",
    "scoring = 'accuracy'\n",
    "results = results=cross_val_score(randomForest, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "scoring = 'neg_log_loss'\n",
    "results = results=cross_val_score(randomForest, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "scoring = 'roc_auc'\n",
    "results = results=cross_val_score(randomForest, x_test, y_test, cv=k_fold, n_jobs=1, scoring=scoring)\n",
    "print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))\n",
    "\n",
    "randomForest.fit(x_train,y_train)\n",
    "predicted=randomForest.predict(x_test)\n",
    "matrix = confusion_matrix(y_test, predicted)\n",
    "print(matrix)\n",
    "\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(matrix, ('False', 'True'), ('False', 'True'))\n",
    "heatmap = sns.heatmap(confusion_matrix_df, annot=True, annot_kws={\"size\": 20}, fmt=\"d\")\n",
    "heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize = 14)\n",
    "heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize = 14)\n",
    "plt.ylabel('True label', fontsize = 14)\n",
    "plt.xlabel('Predicted label', fontsize = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28e25c07f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probs = randomForest.predict_proba(x_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = roc_curve(y_test, preds)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>week_count</th>\n",
       "      <th>week_max_streak</th>\n",
       "      <th>feature_1_mean</th>\n",
       "      <th>feature_1_lifetime_mean</th>\n",
       "      <th>feature_2_mean</th>\n",
       "      <th>feature_2_lifetime_mean</th>\n",
       "      <th>feature_3_mean</th>\n",
       "      <th>feature_3_lifetime_mean</th>\n",
       "      <th>feature_4_mean</th>\n",
       "      <th>feature_4_lifetime_mean</th>\n",
       "      <th>feature_5_mean</th>\n",
       "      <th>feature_5_lifetime_mean</th>\n",
       "      <th>feature_6_mean</th>\n",
       "      <th>feature_6_lifetime_mean</th>\n",
       "      <th>feature_7_mean</th>\n",
       "      <th>feature_7_lifetime_mean</th>\n",
       "      <th>feature_8_mean</th>\n",
       "      <th>feature_8_lifetime_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>courier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3767</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>22.750</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.040700</td>\n",
       "      <td>0.020350</td>\n",
       "      <td>0.959300</td>\n",
       "      <td>0.479650</td>\n",
       "      <td>131.828200</td>\n",
       "      <td>65.914100</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.029500</td>\n",
       "      <td>2107.008875</td>\n",
       "      <td>1053.504437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6282</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>27.500</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>36.375</td>\n",
       "      <td>0.075517</td>\n",
       "      <td>0.056638</td>\n",
       "      <td>0.924483</td>\n",
       "      <td>0.693362</td>\n",
       "      <td>111.291000</td>\n",
       "      <td>83.468250</td>\n",
       "      <td>0.063467</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>4300.183950</td>\n",
       "      <td>3225.137962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10622</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>21.125</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>13.500</td>\n",
       "      <td>0.119050</td>\n",
       "      <td>0.029763</td>\n",
       "      <td>0.880950</td>\n",
       "      <td>0.220238</td>\n",
       "      <td>100.158750</td>\n",
       "      <td>25.039687</td>\n",
       "      <td>0.076200</td>\n",
       "      <td>0.019050</td>\n",
       "      <td>2576.890500</td>\n",
       "      <td>644.222625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13096</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>14.750</td>\n",
       "      <td>89.500000</td>\n",
       "      <td>22.375</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.838400</td>\n",
       "      <td>0.209600</td>\n",
       "      <td>104.296000</td>\n",
       "      <td>26.074000</td>\n",
       "      <td>0.042800</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>4590.275250</td>\n",
       "      <td>1147.568812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14261</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.625000</td>\n",
       "      <td>-3.625</td>\n",
       "      <td>75.500000</td>\n",
       "      <td>75.500</td>\n",
       "      <td>70.875000</td>\n",
       "      <td>70.875</td>\n",
       "      <td>0.018425</td>\n",
       "      <td>0.018425</td>\n",
       "      <td>0.981575</td>\n",
       "      <td>0.981575</td>\n",
       "      <td>115.629900</td>\n",
       "      <td>115.629900</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>0.122925</td>\n",
       "      <td>3870.750225</td>\n",
       "      <td>3870.750225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18869</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.500000</td>\n",
       "      <td>-4.375</td>\n",
       "      <td>117.500000</td>\n",
       "      <td>29.375</td>\n",
       "      <td>61.500000</td>\n",
       "      <td>15.375</td>\n",
       "      <td>0.203600</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.796400</td>\n",
       "      <td>0.199100</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.048800</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>4922.368650</td>\n",
       "      <td>1230.592163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18920</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-19.800000</td>\n",
       "      <td>-12.375</td>\n",
       "      <td>122.600000</td>\n",
       "      <td>76.625</td>\n",
       "      <td>46.800000</td>\n",
       "      <td>29.250</td>\n",
       "      <td>0.064080</td>\n",
       "      <td>0.040050</td>\n",
       "      <td>0.935920</td>\n",
       "      <td>0.584950</td>\n",
       "      <td>113.722480</td>\n",
       "      <td>71.076550</td>\n",
       "      <td>0.112740</td>\n",
       "      <td>0.070462</td>\n",
       "      <td>6995.906260</td>\n",
       "      <td>4372.441412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19450</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>11.125</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.500</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.040175</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.084825</td>\n",
       "      <td>138.357100</td>\n",
       "      <td>17.294638</td>\n",
       "      <td>0.178600</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>8228.571400</td>\n",
       "      <td>1028.571425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39517</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>4.750</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>38.125</td>\n",
       "      <td>86.200000</td>\n",
       "      <td>53.875</td>\n",
       "      <td>0.057800</td>\n",
       "      <td>0.036125</td>\n",
       "      <td>0.942200</td>\n",
       "      <td>0.588875</td>\n",
       "      <td>106.311700</td>\n",
       "      <td>66.444812</td>\n",
       "      <td>0.103880</td>\n",
       "      <td>0.064925</td>\n",
       "      <td>3910.566460</td>\n",
       "      <td>2444.104037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39519</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>5.625</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>43.125</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>26.250</td>\n",
       "      <td>0.142060</td>\n",
       "      <td>0.088788</td>\n",
       "      <td>0.857940</td>\n",
       "      <td>0.536212</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>0.049420</td>\n",
       "      <td>0.030887</td>\n",
       "      <td>3275.958180</td>\n",
       "      <td>2047.473862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41143</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>11.333333</td>\n",
       "      <td>8.500</td>\n",
       "      <td>98.500000</td>\n",
       "      <td>73.875</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>33.375</td>\n",
       "      <td>0.070167</td>\n",
       "      <td>0.052625</td>\n",
       "      <td>0.929833</td>\n",
       "      <td>0.697375</td>\n",
       "      <td>113.196550</td>\n",
       "      <td>84.897413</td>\n",
       "      <td>0.089833</td>\n",
       "      <td>0.067375</td>\n",
       "      <td>6136.813250</td>\n",
       "      <td>4602.609938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41236</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.250</td>\n",
       "      <td>82.333333</td>\n",
       "      <td>30.875</td>\n",
       "      <td>63.333333</td>\n",
       "      <td>23.750</td>\n",
       "      <td>0.069200</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>0.930833</td>\n",
       "      <td>0.349062</td>\n",
       "      <td>105.108867</td>\n",
       "      <td>39.415825</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>0.014225</td>\n",
       "      <td>5374.044833</td>\n",
       "      <td>2015.266813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41486</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.285714</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>119.142857</td>\n",
       "      <td>104.250</td>\n",
       "      <td>56.142857</td>\n",
       "      <td>49.125</td>\n",
       "      <td>0.117714</td>\n",
       "      <td>0.103000</td>\n",
       "      <td>0.882286</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>123.734000</td>\n",
       "      <td>108.267250</td>\n",
       "      <td>0.148114</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>5298.661429</td>\n",
       "      <td>4636.328750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41488</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.750</td>\n",
       "      <td>49.833333</td>\n",
       "      <td>37.375</td>\n",
       "      <td>19.333333</td>\n",
       "      <td>14.500</td>\n",
       "      <td>0.082283</td>\n",
       "      <td>0.061713</td>\n",
       "      <td>0.917717</td>\n",
       "      <td>0.688287</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.009150</td>\n",
       "      <td>2513.667600</td>\n",
       "      <td>1885.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43746</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-4.666667</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>112.500000</td>\n",
       "      <td>84.375</td>\n",
       "      <td>65.333333</td>\n",
       "      <td>49.000</td>\n",
       "      <td>0.141567</td>\n",
       "      <td>0.106175</td>\n",
       "      <td>0.858433</td>\n",
       "      <td>0.643825</td>\n",
       "      <td>110.023667</td>\n",
       "      <td>82.517750</td>\n",
       "      <td>0.066217</td>\n",
       "      <td>0.049662</td>\n",
       "      <td>5419.654717</td>\n",
       "      <td>4064.741037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50907</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>-9.750</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>31.250</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>12.750</td>\n",
       "      <td>0.099333</td>\n",
       "      <td>0.037250</td>\n",
       "      <td>0.900667</td>\n",
       "      <td>0.337750</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>0.048333</td>\n",
       "      <td>0.018125</td>\n",
       "      <td>4746.151900</td>\n",
       "      <td>1779.806962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54968</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>1.875</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>5.500</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>134.500000</td>\n",
       "      <td>16.812500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2258.944400</td>\n",
       "      <td>282.368050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68469</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>12.625</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3.625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>105.517200</td>\n",
       "      <td>13.189650</td>\n",
       "      <td>0.137900</td>\n",
       "      <td>0.017237</td>\n",
       "      <td>3670.517200</td>\n",
       "      <td>458.814650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69348</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.400000</td>\n",
       "      <td>-2.125</td>\n",
       "      <td>66.200000</td>\n",
       "      <td>41.375</td>\n",
       "      <td>41.200000</td>\n",
       "      <td>25.750</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.200563</td>\n",
       "      <td>0.679100</td>\n",
       "      <td>0.424438</td>\n",
       "      <td>103.870180</td>\n",
       "      <td>64.918863</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>0.027412</td>\n",
       "      <td>5241.012780</td>\n",
       "      <td>3275.632988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75053</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>135.333333</td>\n",
       "      <td>101.500</td>\n",
       "      <td>91.166667</td>\n",
       "      <td>68.375</td>\n",
       "      <td>0.121150</td>\n",
       "      <td>0.090862</td>\n",
       "      <td>0.878850</td>\n",
       "      <td>0.659137</td>\n",
       "      <td>107.910700</td>\n",
       "      <td>80.933025</td>\n",
       "      <td>0.045417</td>\n",
       "      <td>0.034063</td>\n",
       "      <td>4438.659967</td>\n",
       "      <td>3328.994975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82968</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>104.333333</td>\n",
       "      <td>78.250</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>59.375</td>\n",
       "      <td>0.056400</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>0.707700</td>\n",
       "      <td>101.467650</td>\n",
       "      <td>76.100738</td>\n",
       "      <td>0.031450</td>\n",
       "      <td>0.023587</td>\n",
       "      <td>4949.336683</td>\n",
       "      <td>3712.002512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86596</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1.166667</td>\n",
       "      <td>-0.875</td>\n",
       "      <td>103.833333</td>\n",
       "      <td>77.875</td>\n",
       "      <td>78.333333</td>\n",
       "      <td>58.750</td>\n",
       "      <td>0.060300</td>\n",
       "      <td>0.045225</td>\n",
       "      <td>0.939700</td>\n",
       "      <td>0.704775</td>\n",
       "      <td>115.074800</td>\n",
       "      <td>86.306100</td>\n",
       "      <td>0.065333</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>2311.378133</td>\n",
       "      <td>1733.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87117</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>-10.000</td>\n",
       "      <td>94.250000</td>\n",
       "      <td>47.125</td>\n",
       "      <td>25.500000</td>\n",
       "      <td>12.750</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.036850</td>\n",
       "      <td>0.926300</td>\n",
       "      <td>0.463150</td>\n",
       "      <td>104.170025</td>\n",
       "      <td>52.085012</td>\n",
       "      <td>0.123375</td>\n",
       "      <td>0.061688</td>\n",
       "      <td>5993.914925</td>\n",
       "      <td>2996.957463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87766</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.600000</td>\n",
       "      <td>-3.500</td>\n",
       "      <td>80.200000</td>\n",
       "      <td>50.125</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>48.750</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.033750</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.591250</td>\n",
       "      <td>107.757880</td>\n",
       "      <td>67.348675</td>\n",
       "      <td>0.038440</td>\n",
       "      <td>0.024025</td>\n",
       "      <td>3803.949200</td>\n",
       "      <td>2377.468250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94233</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.500000</td>\n",
       "      <td>-2.125</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>3.625</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4250.316650</td>\n",
       "      <td>1062.579163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101552</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.125</td>\n",
       "      <td>93.125000</td>\n",
       "      <td>93.125</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>89.000</td>\n",
       "      <td>0.043112</td>\n",
       "      <td>0.043112</td>\n",
       "      <td>0.956888</td>\n",
       "      <td>0.956888</td>\n",
       "      <td>116.890525</td>\n",
       "      <td>116.890525</td>\n",
       "      <td>0.129712</td>\n",
       "      <td>0.129712</td>\n",
       "      <td>2472.698913</td>\n",
       "      <td>2472.698913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103832</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.142857</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>102.142857</td>\n",
       "      <td>89.375</td>\n",
       "      <td>46.714286</td>\n",
       "      <td>40.875</td>\n",
       "      <td>0.091686</td>\n",
       "      <td>0.080225</td>\n",
       "      <td>0.908314</td>\n",
       "      <td>0.794775</td>\n",
       "      <td>100.441629</td>\n",
       "      <td>87.886425</td>\n",
       "      <td>0.015614</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>4509.048800</td>\n",
       "      <td>3945.417700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107223</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.125</td>\n",
       "      <td>42.428571</td>\n",
       "      <td>37.125</td>\n",
       "      <td>61.571429</td>\n",
       "      <td>53.875</td>\n",
       "      <td>0.070414</td>\n",
       "      <td>0.061613</td>\n",
       "      <td>0.929586</td>\n",
       "      <td>0.813388</td>\n",
       "      <td>102.426600</td>\n",
       "      <td>89.623275</td>\n",
       "      <td>0.045757</td>\n",
       "      <td>0.040037</td>\n",
       "      <td>5867.749329</td>\n",
       "      <td>5134.280663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107231</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500</td>\n",
       "      <td>87.800000</td>\n",
       "      <td>54.875</td>\n",
       "      <td>56.600000</td>\n",
       "      <td>35.375</td>\n",
       "      <td>0.160260</td>\n",
       "      <td>0.100163</td>\n",
       "      <td>0.839740</td>\n",
       "      <td>0.524837</td>\n",
       "      <td>100.425820</td>\n",
       "      <td>62.766137</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.053312</td>\n",
       "      <td>4818.853100</td>\n",
       "      <td>3011.783188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107556</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>3.500</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>5.375</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.976700</td>\n",
       "      <td>0.122088</td>\n",
       "      <td>135.023300</td>\n",
       "      <td>16.877913</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.014538</td>\n",
       "      <td>2714.046500</td>\n",
       "      <td>339.255812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515676</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.375</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>12.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3591.700000</td>\n",
       "      <td>448.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515681</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.750</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3723.571400</td>\n",
       "      <td>465.446425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515698</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>4.875</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>37.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4721.794033</td>\n",
       "      <td>1770.672762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515707</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-38.000000</td>\n",
       "      <td>-9.500</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>9.500</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>8.625</td>\n",
       "      <td>0.032250</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.967750</td>\n",
       "      <td>0.241937</td>\n",
       "      <td>100.657900</td>\n",
       "      <td>25.164475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4308.308150</td>\n",
       "      <td>1077.077037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516944</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>18.250</td>\n",
       "      <td>0.007575</td>\n",
       "      <td>0.003788</td>\n",
       "      <td>0.992425</td>\n",
       "      <td>0.496213</td>\n",
       "      <td>110.732775</td>\n",
       "      <td>55.366388</td>\n",
       "      <td>0.158025</td>\n",
       "      <td>0.079012</td>\n",
       "      <td>4061.476225</td>\n",
       "      <td>2030.738113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516968</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>41.750000</td>\n",
       "      <td>20.875</td>\n",
       "      <td>74.250000</td>\n",
       "      <td>37.125</td>\n",
       "      <td>0.048975</td>\n",
       "      <td>0.024487</td>\n",
       "      <td>0.951025</td>\n",
       "      <td>0.475513</td>\n",
       "      <td>104.648100</td>\n",
       "      <td>52.324050</td>\n",
       "      <td>0.017625</td>\n",
       "      <td>0.008813</td>\n",
       "      <td>4031.525450</td>\n",
       "      <td>2015.762725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516974</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>-6.166667</td>\n",
       "      <td>-4.625</td>\n",
       "      <td>36.166667</td>\n",
       "      <td>27.125</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>34.750</td>\n",
       "      <td>0.022200</td>\n",
       "      <td>0.016650</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>0.733350</td>\n",
       "      <td>101.067717</td>\n",
       "      <td>75.800787</td>\n",
       "      <td>0.040467</td>\n",
       "      <td>0.030350</td>\n",
       "      <td>2280.617400</td>\n",
       "      <td>1710.463050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516985</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-35.000000</td>\n",
       "      <td>-8.750</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>9.750</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>10.500</td>\n",
       "      <td>0.020250</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.244938</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.033800</td>\n",
       "      <td>0.008450</td>\n",
       "      <td>2263.106750</td>\n",
       "      <td>565.776687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517018</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>18.600000</td>\n",
       "      <td>11.625</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>15.500</td>\n",
       "      <td>0.014040</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>0.985960</td>\n",
       "      <td>0.616225</td>\n",
       "      <td>104.675860</td>\n",
       "      <td>65.422413</td>\n",
       "      <td>0.074040</td>\n",
       "      <td>0.046275</td>\n",
       "      <td>2350.121000</td>\n",
       "      <td>1468.825625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517029</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>-7.750000</td>\n",
       "      <td>-3.875</td>\n",
       "      <td>34.750000</td>\n",
       "      <td>17.375</td>\n",
       "      <td>48.250000</td>\n",
       "      <td>24.125</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.952200</td>\n",
       "      <td>0.476100</td>\n",
       "      <td>104.751975</td>\n",
       "      <td>52.375988</td>\n",
       "      <td>0.051975</td>\n",
       "      <td>0.025988</td>\n",
       "      <td>4073.751075</td>\n",
       "      <td>2036.875538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517033</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.375</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>2.375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1879.947400</td>\n",
       "      <td>234.993425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517044</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>27.500</td>\n",
       "      <td>46.333333</td>\n",
       "      <td>34.750</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>101.207800</td>\n",
       "      <td>75.905850</td>\n",
       "      <td>0.066833</td>\n",
       "      <td>0.050125</td>\n",
       "      <td>2380.906800</td>\n",
       "      <td>1785.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517058</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>41.833333</td>\n",
       "      <td>31.375</td>\n",
       "      <td>52.166667</td>\n",
       "      <td>39.125</td>\n",
       "      <td>0.058383</td>\n",
       "      <td>0.043788</td>\n",
       "      <td>0.941617</td>\n",
       "      <td>0.706213</td>\n",
       "      <td>104.283667</td>\n",
       "      <td>78.212750</td>\n",
       "      <td>0.057467</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>2513.106250</td>\n",
       "      <td>1884.829687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517073</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.125</td>\n",
       "      <td>48.500000</td>\n",
       "      <td>48.500</td>\n",
       "      <td>57.250000</td>\n",
       "      <td>57.250</td>\n",
       "      <td>0.051725</td>\n",
       "      <td>0.051725</td>\n",
       "      <td>0.948275</td>\n",
       "      <td>0.948275</td>\n",
       "      <td>102.336925</td>\n",
       "      <td>102.336925</td>\n",
       "      <td>0.044225</td>\n",
       "      <td>0.044225</td>\n",
       "      <td>2442.407275</td>\n",
       "      <td>2442.407275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517094</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>2.375</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2.750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>100.769250</td>\n",
       "      <td>25.192312</td>\n",
       "      <td>0.076900</td>\n",
       "      <td>0.019225</td>\n",
       "      <td>4360.807700</td>\n",
       "      <td>1090.201925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517449</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>4.750</td>\n",
       "      <td>43.166667</td>\n",
       "      <td>32.375</td>\n",
       "      <td>69.166667</td>\n",
       "      <td>51.875</td>\n",
       "      <td>0.030467</td>\n",
       "      <td>0.022850</td>\n",
       "      <td>0.969533</td>\n",
       "      <td>0.727150</td>\n",
       "      <td>107.522150</td>\n",
       "      <td>80.641612</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>0.035475</td>\n",
       "      <td>2216.976517</td>\n",
       "      <td>1662.732387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517458</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.875</td>\n",
       "      <td>33.250000</td>\n",
       "      <td>16.625</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>0.006950</td>\n",
       "      <td>0.986100</td>\n",
       "      <td>0.493050</td>\n",
       "      <td>100.642850</td>\n",
       "      <td>50.321425</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>0.017475</td>\n",
       "      <td>4058.451925</td>\n",
       "      <td>2029.225962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517465</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>13.250</td>\n",
       "      <td>20.800000</td>\n",
       "      <td>13.000</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.019187</td>\n",
       "      <td>0.969300</td>\n",
       "      <td>0.605812</td>\n",
       "      <td>104.484200</td>\n",
       "      <td>65.302625</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.105150</td>\n",
       "      <td>2712.765940</td>\n",
       "      <td>1695.478713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517476</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.250</td>\n",
       "      <td>46.625000</td>\n",
       "      <td>46.625</td>\n",
       "      <td>62.375000</td>\n",
       "      <td>62.375</td>\n",
       "      <td>0.046413</td>\n",
       "      <td>0.046413</td>\n",
       "      <td>0.953588</td>\n",
       "      <td>0.953588</td>\n",
       "      <td>100.683638</td>\n",
       "      <td>100.683638</td>\n",
       "      <td>0.055950</td>\n",
       "      <td>0.055950</td>\n",
       "      <td>3824.999950</td>\n",
       "      <td>3824.999950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517495</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.500000</td>\n",
       "      <td>-2.625</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>5.125</td>\n",
       "      <td>27.500000</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.197750</td>\n",
       "      <td>0.049438</td>\n",
       "      <td>0.802250</td>\n",
       "      <td>0.200562</td>\n",
       "      <td>100.370350</td>\n",
       "      <td>25.092588</td>\n",
       "      <td>0.036350</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>5484.841250</td>\n",
       "      <td>1371.210312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517510</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>-9.200000</td>\n",
       "      <td>-5.750</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>11.250</td>\n",
       "      <td>29.600000</td>\n",
       "      <td>18.500</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.032563</td>\n",
       "      <td>0.947900</td>\n",
       "      <td>0.592438</td>\n",
       "      <td>100.571420</td>\n",
       "      <td>62.857138</td>\n",
       "      <td>0.117960</td>\n",
       "      <td>0.073725</td>\n",
       "      <td>2551.913980</td>\n",
       "      <td>1594.946238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517517</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.250</td>\n",
       "      <td>33.800000</td>\n",
       "      <td>21.125</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>34.375</td>\n",
       "      <td>0.045720</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.954280</td>\n",
       "      <td>0.596425</td>\n",
       "      <td>123.578060</td>\n",
       "      <td>77.236288</td>\n",
       "      <td>0.127140</td>\n",
       "      <td>0.079463</td>\n",
       "      <td>3882.039740</td>\n",
       "      <td>2426.274838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518659</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>31.250</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>43.000</td>\n",
       "      <td>0.058417</td>\n",
       "      <td>0.043812</td>\n",
       "      <td>0.941583</td>\n",
       "      <td>0.706187</td>\n",
       "      <td>101.245433</td>\n",
       "      <td>75.934075</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.025950</td>\n",
       "      <td>4891.840767</td>\n",
       "      <td>3668.880575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518704</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>44.500000</td>\n",
       "      <td>44.500</td>\n",
       "      <td>59.375000</td>\n",
       "      <td>59.375</td>\n",
       "      <td>0.056050</td>\n",
       "      <td>0.056050</td>\n",
       "      <td>0.943950</td>\n",
       "      <td>0.943950</td>\n",
       "      <td>102.623638</td>\n",
       "      <td>102.623638</td>\n",
       "      <td>0.044388</td>\n",
       "      <td>0.044388</td>\n",
       "      <td>4852.564950</td>\n",
       "      <td>4852.564950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519141</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>-1.833333</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>23.166667</td>\n",
       "      <td>17.375</td>\n",
       "      <td>30.833333</td>\n",
       "      <td>23.125</td>\n",
       "      <td>0.043717</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.956283</td>\n",
       "      <td>0.717213</td>\n",
       "      <td>112.476167</td>\n",
       "      <td>84.357125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4326.150017</td>\n",
       "      <td>3244.612512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519286</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-0.625</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>3.750</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.222200</td>\n",
       "      <td>0.027775</td>\n",
       "      <td>0.777800</td>\n",
       "      <td>0.097225</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.166700</td>\n",
       "      <td>0.020837</td>\n",
       "      <td>4138.388900</td>\n",
       "      <td>517.298612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519301</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>28.500000</td>\n",
       "      <td>21.375</td>\n",
       "      <td>40.166667</td>\n",
       "      <td>30.125</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>0.983817</td>\n",
       "      <td>0.737862</td>\n",
       "      <td>107.420683</td>\n",
       "      <td>80.565513</td>\n",
       "      <td>0.124550</td>\n",
       "      <td>0.093413</td>\n",
       "      <td>2409.970250</td>\n",
       "      <td>1807.477688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519307</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>10.625</td>\n",
       "      <td>26.500000</td>\n",
       "      <td>13.250</td>\n",
       "      <td>0.115825</td>\n",
       "      <td>0.057912</td>\n",
       "      <td>0.884175</td>\n",
       "      <td>0.442088</td>\n",
       "      <td>101.180550</td>\n",
       "      <td>50.590275</td>\n",
       "      <td>0.144525</td>\n",
       "      <td>0.072263</td>\n",
       "      <td>3820.462075</td>\n",
       "      <td>1910.231037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519324</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.500000</td>\n",
       "      <td>-5.250</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>32.125</td>\n",
       "      <td>59.750000</td>\n",
       "      <td>29.875</td>\n",
       "      <td>0.104275</td>\n",
       "      <td>0.052138</td>\n",
       "      <td>0.895725</td>\n",
       "      <td>0.447862</td>\n",
       "      <td>112.476000</td>\n",
       "      <td>56.238000</td>\n",
       "      <td>0.088100</td>\n",
       "      <td>0.044050</td>\n",
       "      <td>3357.930550</td>\n",
       "      <td>1678.965275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519341</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>-14.666667</td>\n",
       "      <td>-5.500</td>\n",
       "      <td>25.333333</td>\n",
       "      <td>9.500</td>\n",
       "      <td>37.333333</td>\n",
       "      <td>14.000</td>\n",
       "      <td>0.082867</td>\n",
       "      <td>0.031075</td>\n",
       "      <td>0.917133</td>\n",
       "      <td>0.343925</td>\n",
       "      <td>101.027100</td>\n",
       "      <td>37.885162</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>2610.667167</td>\n",
       "      <td>979.000187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>729 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y  week_count  week_max_streak  feature_1_mean  \\\n",
       "courier                                                   \n",
       "3767     1           4                3        1.750000   \n",
       "6282     0           6                6        1.333333   \n",
       "10622    1           2                2       -3.500000   \n",
       "13096    0           2                2        0.000000   \n",
       "14261    1           8                8       -3.625000   \n",
       "18869    1           2                1      -17.500000   \n",
       "18920    1           5                5      -19.800000   \n",
       "19450    0           1                1       52.000000   \n",
       "39517    0           5                3        7.600000   \n",
       "39519    0           5                3        9.000000   \n",
       "41143    0           6                3       11.333333   \n",
       "41236    1           3                2        3.333333   \n",
       "41486    0           7                6       -2.285714   \n",
       "41488    0           6                4        5.000000   \n",
       "43746    0           6                4       -4.666667   \n",
       "50907    0           3                2      -26.000000   \n",
       "54968    1           1                1       15.000000   \n",
       "68469    0           1                1        7.000000   \n",
       "69348    0           5                4       -3.400000   \n",
       "75053    1           6                5       -2.666667   \n",
       "82968    1           6                4       -0.666667   \n",
       "86596    0           6                5       -1.166667   \n",
       "87117    0           4                2      -20.000000   \n",
       "87766    0           5                5       -5.600000   \n",
       "94233    1           2                2       -8.500000   \n",
       "101552   0           8                8        1.125000   \n",
       "103832   0           7                6       -1.142857   \n",
       "107223   0           7                6        0.142857   \n",
       "107231   1           5                2        4.000000   \n",
       "107556   0           1                1      -10.000000   \n",
       "...     ..         ...              ...             ...   \n",
       "515676   1           1                1      -11.000000   \n",
       "515681   1           1                1        7.000000   \n",
       "515698   1           3                3       -2.666667   \n",
       "515707   1           2                1      -38.000000   \n",
       "516944   0           4                4        0.250000   \n",
       "516968   0           4                3        8.000000   \n",
       "516974   0           6                3       -6.166667   \n",
       "516985   1           2                1      -35.000000   \n",
       "517018   0           5                2       -3.000000   \n",
       "517029   0           4                3       -7.750000   \n",
       "517033   1           1                1        0.000000   \n",
       "517044   1           6                6       -0.166667   \n",
       "517058   1           6                6        2.000000   \n",
       "517073   0           8                8        1.125000   \n",
       "517094   0           2                1       -5.000000   \n",
       "517449   0           6                5        6.333333   \n",
       "517458   1           4                2        1.000000   \n",
       "517465   0           5                2        0.800000   \n",
       "517476   0           8                8        2.250000   \n",
       "517495   1           2                2      -10.500000   \n",
       "517510   1           5                3       -9.200000   \n",
       "517517   0           5                2       -2.000000   \n",
       "518659   1           6                4        1.333333   \n",
       "518704   0           8                8        1.250000   \n",
       "519141   1           6                6       -1.833333   \n",
       "519286   0           1                1       -5.000000   \n",
       "519301   0           6                4       -0.500000   \n",
       "519307   0           4                2       -0.750000   \n",
       "519324   1           4                2      -10.500000   \n",
       "519341   1           3                2      -14.666667   \n",
       "\n",
       "         feature_1_lifetime_mean  feature_2_mean  feature_2_lifetime_mean  \\\n",
       "courier                                                                     \n",
       "3767                       0.875       45.500000                   22.750   \n",
       "6282                       1.000       36.666667                   27.500   \n",
       "10622                     -0.875       84.500000                   21.125   \n",
       "13096                      0.000       59.000000                   14.750   \n",
       "14261                     -3.625       75.500000                   75.500   \n",
       "18869                     -4.375      117.500000                   29.375   \n",
       "18920                    -12.375      122.600000                   76.625   \n",
       "19450                      6.500       89.000000                   11.125   \n",
       "39517                      4.750       61.000000                   38.125   \n",
       "39519                      5.625       69.000000                   43.125   \n",
       "41143                      8.500       98.500000                   73.875   \n",
       "41236                      1.250       82.333333                   30.875   \n",
       "41486                     -2.000      119.142857                  104.250   \n",
       "41488                      3.750       49.833333                   37.375   \n",
       "43746                     -3.500      112.500000                   84.375   \n",
       "50907                     -9.750       83.333333                   31.250   \n",
       "54968                      1.875       44.000000                    5.500   \n",
       "68469                      0.875      101.000000                   12.625   \n",
       "69348                     -2.125       66.200000                   41.375   \n",
       "75053                     -2.000      135.333333                  101.500   \n",
       "82968                     -0.500      104.333333                   78.250   \n",
       "86596                     -0.875      103.833333                   77.875   \n",
       "87117                    -10.000       94.250000                   47.125   \n",
       "87766                     -3.500       80.200000                   50.125   \n",
       "94233                     -2.125       14.500000                    3.625   \n",
       "101552                     1.125       93.125000                   93.125   \n",
       "103832                    -1.000      102.142857                   89.375   \n",
       "107223                     0.125       42.428571                   37.125   \n",
       "107231                     2.500       87.800000                   54.875   \n",
       "107556                    -1.250       28.000000                    3.500   \n",
       "...                          ...             ...                      ...   \n",
       "515676                    -1.375       11.000000                    1.375   \n",
       "515681                     0.875        6.000000                    0.750   \n",
       "515698                    -1.000       13.000000                    4.875   \n",
       "515707                    -9.500       38.000000                    9.500   \n",
       "516944                     0.125       22.000000                   11.000   \n",
       "516968                     4.000       41.750000                   20.875   \n",
       "516974                    -4.625       36.166667                   27.125   \n",
       "516985                    -8.750       39.000000                    9.750   \n",
       "517018                    -1.875       18.600000                   11.625   \n",
       "517029                    -3.875       34.750000                   17.375   \n",
       "517033                     0.000       11.000000                    1.375   \n",
       "517044                    -0.125       36.666667                   27.500   \n",
       "517058                     1.500       41.833333                   31.375   \n",
       "517073                     1.125       48.500000                   48.500   \n",
       "517094                    -1.250        9.500000                    2.375   \n",
       "517449                     4.750       43.166667                   32.375   \n",
       "517458                     0.500       25.750000                   12.875   \n",
       "517465                     0.500       21.200000                   13.250   \n",
       "517476                     2.250       46.625000                   46.625   \n",
       "517495                    -2.625       20.500000                    5.125   \n",
       "517510                    -5.750       18.000000                   11.250   \n",
       "517517                    -1.250       33.800000                   21.125   \n",
       "518659                     1.000       41.666667                   31.250   \n",
       "518704                     1.250       44.500000                   44.500   \n",
       "519141                    -1.375       23.166667                   17.375   \n",
       "519286                    -0.625       30.000000                    3.750   \n",
       "519301                    -0.375       28.500000                   21.375   \n",
       "519307                    -0.375       21.250000                   10.625   \n",
       "519324                    -5.250       64.250000                   32.125   \n",
       "519341                    -5.500       25.333333                    9.500   \n",
       "\n",
       "         feature_3_mean  feature_3_lifetime_mean  feature_4_mean  \\\n",
       "courier                                                            \n",
       "3767          46.000000                   23.000        0.040700   \n",
       "6282          48.500000                   36.375        0.075517   \n",
       "10622         54.000000                   13.500        0.119050   \n",
       "13096         89.500000                   22.375        0.161600   \n",
       "14261         70.875000                   70.875        0.018425   \n",
       "18869         61.500000                   15.375        0.203600   \n",
       "18920         46.800000                   29.250        0.064080   \n",
       "19450         28.000000                    3.500        0.321400   \n",
       "39517         86.200000                   53.875        0.057800   \n",
       "39519         42.000000                   26.250        0.142060   \n",
       "41143         44.500000                   33.375        0.070167   \n",
       "41236         63.333333                   23.750        0.069200   \n",
       "41486         56.142857                   49.125        0.117714   \n",
       "41488         19.333333                   14.500        0.082283   \n",
       "43746         65.333333                   49.000        0.141567   \n",
       "50907         34.000000                   12.750        0.099333   \n",
       "54968         18.000000                    2.250        0.000000   \n",
       "68469         29.000000                    3.625        0.000000   \n",
       "69348         41.200000                   25.750        0.320900   \n",
       "75053         91.166667                   68.375        0.121150   \n",
       "82968         79.166667                   59.375        0.056400   \n",
       "86596         78.333333                   58.750        0.060300   \n",
       "87117         25.500000                   12.750        0.073700   \n",
       "87766         78.000000                   48.750        0.054000   \n",
       "94233          8.000000                    2.000        0.000000   \n",
       "101552        89.000000                   89.000        0.043112   \n",
       "103832        46.714286                   40.875        0.091686   \n",
       "107223        61.571429                   53.875        0.070414   \n",
       "107231        56.600000                   35.375        0.160260   \n",
       "107556        43.000000                    5.375        0.023300   \n",
       "...                 ...                      ...             ...   \n",
       "515676        10.000000                    1.250        0.100000   \n",
       "515681         7.000000                    0.875        0.000000   \n",
       "515698         9.000000                    3.375        0.000000   \n",
       "515707        34.500000                    8.625        0.032250   \n",
       "516944        36.500000                   18.250        0.007575   \n",
       "516968        74.250000                   37.125        0.048975   \n",
       "516974        46.333333                   34.750        0.022200   \n",
       "516985        42.000000                   10.500        0.020250   \n",
       "517018        24.800000                   15.500        0.014040   \n",
       "517029        48.250000                   24.125        0.047800   \n",
       "517033        19.000000                    2.375        0.000000   \n",
       "517044        46.333333                   34.750        0.064000   \n",
       "517058        52.166667                   39.125        0.058383   \n",
       "517073        57.250000                   57.250        0.051725   \n",
       "517094        11.000000                    2.750        0.000000   \n",
       "517449        69.166667                   51.875        0.030467   \n",
       "517458        33.250000                   16.625        0.013900   \n",
       "517465        20.800000                   13.000        0.030700   \n",
       "517476        62.375000                   62.375        0.046413   \n",
       "517495        27.500000                    6.875        0.197750   \n",
       "517510        29.600000                   18.500        0.052100   \n",
       "517517        55.000000                   34.375        0.045720   \n",
       "518659        57.333333                   43.000        0.058417   \n",
       "518704        59.375000                   59.375        0.056050   \n",
       "519141        30.833333                   23.125        0.043717   \n",
       "519286        18.000000                    2.250        0.222200   \n",
       "519301        40.166667                   30.125        0.016200   \n",
       "519307        26.500000                   13.250        0.115825   \n",
       "519324        59.750000                   29.875        0.104275   \n",
       "519341        37.333333                   14.000        0.082867   \n",
       "\n",
       "         feature_4_lifetime_mean  feature_5_mean  feature_5_lifetime_mean  \\\n",
       "courier                                                                     \n",
       "3767                    0.020350        0.959300                 0.479650   \n",
       "6282                    0.056638        0.924483                 0.693362   \n",
       "10622                   0.029763        0.880950                 0.220238   \n",
       "13096                   0.040400        0.838400                 0.209600   \n",
       "14261                   0.018425        0.981575                 0.981575   \n",
       "18869                   0.050900        0.796400                 0.199100   \n",
       "18920                   0.040050        0.935920                 0.584950   \n",
       "19450                   0.040175        0.678600                 0.084825   \n",
       "39517                   0.036125        0.942200                 0.588875   \n",
       "39519                   0.088788        0.857940                 0.536212   \n",
       "41143                   0.052625        0.929833                 0.697375   \n",
       "41236                   0.025950        0.930833                 0.349062   \n",
       "41486                   0.103000        0.882286                 0.772000   \n",
       "41488                   0.061713        0.917717                 0.688287   \n",
       "43746                   0.106175        0.858433                 0.643825   \n",
       "50907                   0.037250        0.900667                 0.337750   \n",
       "54968                   0.000000        1.000000                 0.125000   \n",
       "68469                   0.000000        1.000000                 0.125000   \n",
       "69348                   0.200563        0.679100                 0.424438   \n",
       "75053                   0.090862        0.878850                 0.659137   \n",
       "82968                   0.042300        0.943600                 0.707700   \n",
       "86596                   0.045225        0.939700                 0.704775   \n",
       "87117                   0.036850        0.926300                 0.463150   \n",
       "87766                   0.033750        0.946000                 0.591250   \n",
       "94233                   0.000000        1.000000                 0.250000   \n",
       "101552                  0.043112        0.956888                 0.956888   \n",
       "103832                  0.080225        0.908314                 0.794775   \n",
       "107223                  0.061613        0.929586                 0.813388   \n",
       "107231                  0.100163        0.839740                 0.524837   \n",
       "107556                  0.002913        0.976700                 0.122088   \n",
       "...                          ...             ...                      ...   \n",
       "515676                  0.012500        0.900000                 0.112500   \n",
       "515681                  0.000000        1.000000                 0.125000   \n",
       "515698                  0.000000        1.000000                 0.375000   \n",
       "515707                  0.008063        0.967750                 0.241937   \n",
       "516944                  0.003788        0.992425                 0.496213   \n",
       "516968                  0.024487        0.951025                 0.475513   \n",
       "516974                  0.016650        0.977800                 0.733350   \n",
       "516985                  0.005063        0.979750                 0.244938   \n",
       "517018                  0.008775        0.985960                 0.616225   \n",
       "517029                  0.023900        0.952200                 0.476100   \n",
       "517033                  0.000000        1.000000                 0.125000   \n",
       "517044                  0.048000        0.936000                 0.702000   \n",
       "517058                  0.043788        0.941617                 0.706213   \n",
       "517073                  0.051725        0.948275                 0.948275   \n",
       "517094                  0.000000        1.000000                 0.250000   \n",
       "517449                  0.022850        0.969533                 0.727150   \n",
       "517458                  0.006950        0.986100                 0.493050   \n",
       "517465                  0.019187        0.969300                 0.605812   \n",
       "517476                  0.046413        0.953588                 0.953588   \n",
       "517495                  0.049438        0.802250                 0.200562   \n",
       "517510                  0.032563        0.947900                 0.592438   \n",
       "517517                  0.028575        0.954280                 0.596425   \n",
       "518659                  0.043812        0.941583                 0.706187   \n",
       "518704                  0.056050        0.943950                 0.943950   \n",
       "519141                  0.032787        0.956283                 0.717213   \n",
       "519286                  0.027775        0.777800                 0.097225   \n",
       "519301                  0.012150        0.983817                 0.737862   \n",
       "519307                  0.057912        0.884175                 0.442088   \n",
       "519324                  0.052138        0.895725                 0.447862   \n",
       "519341                  0.031075        0.917133                 0.343925   \n",
       "\n",
       "         feature_6_mean  feature_6_lifetime_mean  feature_7_mean  \\\n",
       "courier                                                            \n",
       "3767         131.828200                65.914100        0.059000   \n",
       "6282         111.291000                83.468250        0.063467   \n",
       "10622        100.158750                25.039687        0.076200   \n",
       "13096        104.296000                26.074000        0.042800   \n",
       "14261        115.629900               115.629900        0.122925   \n",
       "18869        100.000000                25.000000        0.048800   \n",
       "18920        113.722480                71.076550        0.112740   \n",
       "19450        138.357100                17.294638        0.178600   \n",
       "39517        106.311700                66.444812        0.103880   \n",
       "39519        100.000000                62.500000        0.049420   \n",
       "41143        113.196550                84.897413        0.089833   \n",
       "41236        105.108867                39.415825        0.037933   \n",
       "41486        123.734000               108.267250        0.148114   \n",
       "41488        100.000000                75.000000        0.012200   \n",
       "43746        110.023667                82.517750        0.066217   \n",
       "50907        100.000000                37.500000        0.048333   \n",
       "54968        134.500000                16.812500        0.000000   \n",
       "68469        105.517200                13.189650        0.137900   \n",
       "69348        103.870180                64.918863        0.043860   \n",
       "75053        107.910700                80.933025        0.045417   \n",
       "82968        101.467650                76.100738        0.031450   \n",
       "86596        115.074800                86.306100        0.065333   \n",
       "87117        104.170025                52.085012        0.123375   \n",
       "87766        107.757880                67.348675        0.038440   \n",
       "94233        100.000000                25.000000        0.000000   \n",
       "101552       116.890525               116.890525        0.129712   \n",
       "103832       100.441629                87.886425        0.015614   \n",
       "107223       102.426600                89.623275        0.045757   \n",
       "107231       100.425820                62.766137        0.085300   \n",
       "107556       135.023300                16.877913        0.116300   \n",
       "...                 ...                      ...             ...   \n",
       "515676       102.000000                12.750000        0.000000   \n",
       "515681       100.000000                12.500000        0.000000   \n",
       "515698       100.000000                37.500000        0.000000   \n",
       "515707       100.657900                25.164475        0.000000   \n",
       "516944       110.732775                55.366388        0.158025   \n",
       "516968       104.648100                52.324050        0.017625   \n",
       "516974       101.067717                75.800787        0.040467   \n",
       "516985       100.000000                25.000000        0.033800   \n",
       "517018       104.675860                65.422413        0.074040   \n",
       "517029       104.751975                52.375988        0.051975   \n",
       "517033       100.000000                12.500000        0.000000   \n",
       "517044       101.207800                75.905850        0.066833   \n",
       "517058       104.283667                78.212750        0.057467   \n",
       "517073       102.336925               102.336925        0.044225   \n",
       "517094       100.769250                25.192312        0.076900   \n",
       "517449       107.522150                80.641612        0.047300   \n",
       "517458       100.642850                50.321425        0.034950   \n",
       "517465       104.484200                65.302625        0.168240   \n",
       "517476       100.683638               100.683638        0.055950   \n",
       "517495       100.370350                25.092588        0.036350   \n",
       "517510       100.571420                62.857138        0.117960   \n",
       "517517       123.578060                77.236288        0.127140   \n",
       "518659       101.245433                75.934075        0.034600   \n",
       "518704       102.623638               102.623638        0.044388   \n",
       "519141       112.476167                84.357125        0.000000   \n",
       "519286       100.000000                12.500000        0.166700   \n",
       "519301       107.420683                80.565513        0.124550   \n",
       "519307       101.180550                50.590275        0.144525   \n",
       "519324       112.476000                56.238000        0.088100   \n",
       "519341       101.027100                37.885162        0.062500   \n",
       "\n",
       "         feature_7_lifetime_mean  feature_8_mean  feature_8_lifetime_mean  \n",
       "courier                                                                    \n",
       "3767                    0.029500     2107.008875              1053.504437  \n",
       "6282                    0.047600     4300.183950              3225.137962  \n",
       "10622                   0.019050     2576.890500               644.222625  \n",
       "13096                   0.010700     4590.275250              1147.568812  \n",
       "14261                   0.122925     3870.750225              3870.750225  \n",
       "18869                   0.012200     4922.368650              1230.592163  \n",
       "18920                   0.070462     6995.906260              4372.441412  \n",
       "19450                   0.022325     8228.571400              1028.571425  \n",
       "39517                   0.064925     3910.566460              2444.104037  \n",
       "39519                   0.030887     3275.958180              2047.473862  \n",
       "41143                   0.067375     6136.813250              4602.609938  \n",
       "41236                   0.014225     5374.044833              2015.266813  \n",
       "41486                   0.129600     5298.661429              4636.328750  \n",
       "41488                   0.009150     2513.667600              1885.250700  \n",
       "43746                   0.049662     5419.654717              4064.741037  \n",
       "50907                   0.018125     4746.151900              1779.806962  \n",
       "54968                   0.000000     2258.944400               282.368050  \n",
       "68469                   0.017237     3670.517200               458.814650  \n",
       "69348                   0.027412     5241.012780              3275.632988  \n",
       "75053                   0.034063     4438.659967              3328.994975  \n",
       "82968                   0.023587     4949.336683              3712.002512  \n",
       "86596                   0.049000     2311.378133              1733.533600  \n",
       "87117                   0.061688     5993.914925              2996.957463  \n",
       "87766                   0.024025     3803.949200              2377.468250  \n",
       "94233                   0.000000     4250.316650              1062.579163  \n",
       "101552                  0.129712     2472.698913              2472.698913  \n",
       "103832                  0.013662     4509.048800              3945.417700  \n",
       "107223                  0.040037     5867.749329              5134.280663  \n",
       "107231                  0.053312     4818.853100              3011.783188  \n",
       "107556                  0.014538     2714.046500               339.255812  \n",
       "...                          ...             ...                      ...  \n",
       "515676                  0.000000     3591.700000               448.962500  \n",
       "515681                  0.000000     3723.571400               465.446425  \n",
       "515698                  0.000000     4721.794033              1770.672762  \n",
       "515707                  0.000000     4308.308150              1077.077037  \n",
       "516944                  0.079012     4061.476225              2030.738113  \n",
       "516968                  0.008813     4031.525450              2015.762725  \n",
       "516974                  0.030350     2280.617400              1710.463050  \n",
       "516985                  0.008450     2263.106750               565.776687  \n",
       "517018                  0.046275     2350.121000              1468.825625  \n",
       "517029                  0.025988     4073.751075              2036.875538  \n",
       "517033                  0.000000     1879.947400               234.993425  \n",
       "517044                  0.050125     2380.906800              1785.680100  \n",
       "517058                  0.043100     2513.106250              1884.829687  \n",
       "517073                  0.044225     2442.407275              2442.407275  \n",
       "517094                  0.019225     4360.807700              1090.201925  \n",
       "517449                  0.035475     2216.976517              1662.732387  \n",
       "517458                  0.017475     4058.451925              2029.225962  \n",
       "517465                  0.105150     2712.765940              1695.478713  \n",
       "517476                  0.055950     3824.999950              3824.999950  \n",
       "517495                  0.009088     5484.841250              1371.210312  \n",
       "517510                  0.073725     2551.913980              1594.946238  \n",
       "517517                  0.079463     3882.039740              2426.274838  \n",
       "518659                  0.025950     4891.840767              3668.880575  \n",
       "518704                  0.044388     4852.564950              4852.564950  \n",
       "519141                  0.000000     4326.150017              3244.612512  \n",
       "519286                  0.020837     4138.388900               517.298612  \n",
       "519301                  0.093413     2409.970250              1807.477688  \n",
       "519307                  0.072263     3820.462075              1910.231037  \n",
       "519324                  0.044050     3357.930550              1678.965275  \n",
       "519341                  0.023438     2610.667167               979.000187  \n",
       "\n",
       "[729 rows x 19 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
